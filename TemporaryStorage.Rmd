---
title: "TemporaryStorage"
output: html_document
date: "2025-11-11"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

# Transformations of Variables

-   If data is not normally distributed, we need to conduct a transformation. When we transform a variable, we hope to change the shape to normal so that we can continue to calculate under the rules of the normal distribution. For variables that are right skewed, a few transformations that could work to make the variable more normally distributed are: square root, cube root, reciprocal, and log.

-   First, read in the opioid data set from so we can see a variable that is considered not normal.\

```{r}
# Distance to substance abuse facility with medication-assisted treatment
dist.mat <- read.csv("data/opioidFacility.csv")
# Review the data
summary(dist.mat)
```

```{r, fig.alt = "Histogram Example of VALUE Variable"}
# Graph the distance variable which is called Value but represents miles. 
# Note that this graph does not look normal - instead, it looks right or positive skewed. 
dist.mat %>%
  ggplot(aes(VALUE)) +
  geom_histogram(fill = "#7463AC", color = "white") +
  theme_minimal() +
  labs(x = "Miles to nearest substance abuse facility", y = "Number of counties")
```

-   Next, transform the variable to the 4 recommended transformations to see which one works best. We cannot see that result yet until we graph these results.

    -   This requires 4 separate calculations using mutate() commands.

    ```{r}
    dist.mat.cleaned <- dist.mat %>%
      mutate(miles.cube.root = VALUE^(1/3)) %>%
      mutate(miles.log = log(x = VALUE)) %>%
      mutate(miles.inverse = 1/VALUE) %>%
      mutate(miles.sqrt = sqrt(x = VALUE))
    ```

-   Now, graph the variable with the 4 recommended transformations to see which is most normal (bell shaped).

```{r}
cuberoot <- dist.mat.cleaned %>%
  ggplot(aes(x = miles.cube.root)) +
  geom_histogram(fill = "#7463AC", color = "white") +
  theme_minimal() +
  labs(x = "Cube root of miles to nearest facility", y = "Number of counties")

logged <- dist.mat.cleaned %>%
  ggplot(aes(x = miles.log)) +
  geom_histogram(fill = "#7463AC", color = "white") +
  theme_minimal() +
  labs(x = "Log of miles to nearest facility", y = "")

inversed <- dist.mat.cleaned %>%
  ggplot(aes(x = miles.inverse)) +
  geom_histogram(fill = "#7463AC", color = "white") +
  theme_minimal() + xlim(0, 1) +
  labs(x = "Inverse of miles to nearest facility", y = "Number of counties")

squareroot <- dist.mat.cleaned %>%
  ggplot(aes(x = miles.sqrt)) +
  geom_histogram(fill = "#7463AC", color = "white") +
  theme_minimal() +
  labs(x = "Square root of miles to nearest facility", y = "")
```

-   We can show all 4 graphs at one time to directly compare. Ensure your plot window is large enough to see this.

```{r, message=FALSE, fig.alt = "Histogram of 4 Transformations"}
gridExtra::grid.arrange(cuberoot, logged, inversed, squareroot)
```

-   Finally, determine if any of the transformations help. In this example, we determined that the cuberoot had the most normal transformation. The cube root graph contains a nice bell shape curve.

-   Let's use that new variable in the analysis. Start by summarizing the descriptive statistics, including retrieving the mean and standard deviation for cube root of miles, which are values that are required in the probability calculations.

```{r}
dist.mat.cleaned %>%
  drop_na(miles.cube.root) %>%
  summarize(mean.tran.dist = mean(x = miles.cube.root),sd.tran.dist = sd(x = miles.cube.root))
```

-   2.66 and .79 are the values we pulled for mean and standard deviation. We can use that information to calculate probabilities based on the functions we mentioned above.
-   So, what happens if the cuberoot of X \< 3 or less than 27 miles from the facility?
-   We estimate that about 66% of counties fall in the shaded area, having to travel less than 27 miles to nearest facility (27 = 3\^3).
-   This means that (1- 0.6665403)\*100 is the percentage of countries having to travel more than 27 miles to the nearest facility.

```{r}
27^(1/3)
3^3

# P(X< cuberoot(27) = P(X < 3)
pnorm(3, 2.66, .79) ##about 66% likely

# P(X > 3) #about 33% likely
pnorm(3, 2.66, .79, lower.tail = FALSE)
1-pnorm(3, 2.66, .79)
```

-   We estimate that about 20% of counties fall in the shaded area, having to travel \< 8 miles to nearest facility (8 = 2\^3).

```{r}
pnorm(2, 2.66, .79)
```

-   We can use the equation to calculate the z-score for a county where you have to drive 15 miles to a facility.

```{r}
##z = (x-m)/sd since we are in cube root - we multiply x by ^1/3
(15^(1/3) - 2.66)/0.79
```

-   The transformed distance of a facility 15 miles away is .24 standard deviations LOWER than the mean transformed distance.

-   Next, we can calculate z for a county with residents who have to travel 50 miles to the nearest facility. In the transformed miles variable, this would be the cube root of 50, or a value of 3.68.

```{r}
(50^(1/3)-2.66)/0.79 #[1] 1.296242
```

-   This indicated that the transformed distance to a facility with MAT for this example county was 1.29 standard deviations above the mean transformed distance from a county to a facility with MAT.

### Transformation Second Example

-   Taking a second example, let us look at the PHYSHLTH variable from the gender dataset (brfss.csv). We worked with this dataset in an earlier lesson. In doing so, we cleaned the data.
-   I copied over that data preparation code in regards to the variable of interest (PHYSHLTH), and tidied it up for one example. To remind ourselves, the question being asked was the following, "Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good?"
-   If ever you are using the MASS package and dplyr, the select function may have a conflict where R does not know which to use. If you get an error when using select, add dplyr:: in front of the statement to ensure you are using select from dplyr to select variables.

```{r}
# 
gender <- read.csv("data/brfss.csv")
# Review the data
summary(gender)

# PHYSHLTH example
gender.clean <- gender %>% 
     dplyr::select(PHYSHLTH) %>%
     drop_na() %>%
     # Turn the 77 values to NA, since 77 meant don't know or not sure from the brss codebook
     mutate(PHYSHLTH = na_if(PHYSHLTH, y=77)) %>%     
     # Turn the 99 values to NA, since 99 meant Refuled from the brss codebook. 
     mutate(PHYSHLTH = na_if(PHYSHLTH, y=99)) %>%
     # Recode the 88 values to 0 - since the number 88 meant 0 days of illness from the brss codebook. 
     mutate(PHYSHLTH = recode(PHYSHLTH, '88'=0L))
table(gender.clean$PHYSHLTH)


summary(gender.clean)
qnorm(0.95, mean = 500, sd = 10)
```

-   Once here, we graph PHYSHLTH.\

```{r, fig.alt="Histogram PHYSHLTH"}
gender.clean %>%
     ggplot(aes(PHYSHLTH)) +
     geom_histogram(fill = "#7463AC", color = "white") +
     theme_minimal() +
     labs(x = "Number of Days Sick", y = "Frequency")
```

-   We determined from the descriptive statistics lesson that this variable had severe skewness (positive). Most people had 0 days of illness.

-   Next, we run the 4 calculations by mutating the variable and saving all 4 transformation under new variable names.

```{r}
genderTransform <- gender.clean %>%
     mutate(phy.cube.root = PHYSHLTH^(1/3)) %>%
     mutate(phy.log = log(x = PHYSHLTH)) %>%
     mutate(phy.inverse = 1/PHYSHLTH) %>%
     mutate(phy.sqrt = sqrt(x = PHYSHLTH))
```

-   Next, we create the 4 graphs for each of the 4 transformations labelled above to see if one helps.

```{r}
cuberoot <- genderTransform %>%
     ggplot(aes(x = phy.cube.root)) +
     geom_histogram(fill = "#7463AC", color = "white",
                    binwidth = .5) +
     theme_minimal() +
     labs(x = "Cube root", y = "")
logged <- genderTransform %>%
     ggplot(aes(x = phy.log)) +
     geom_histogram(fill = "#7463AC", color = "white",
                    binwidth = .5) +
     theme_minimal() +
     labs(x = "Log", y = "")
inversed <- genderTransform %>%
     ggplot(aes(x = phy.inverse)) + xlim(0,1)+
     geom_histogram(fill = "#7463AC", color = "white",
                    binwidth = .05) +
     theme_minimal() +
     labs(x = "Inverse", y = "")
squareroot <- genderTransform %>%
     ggplot(aes(x = phy.sqrt)) +
     geom_histogram(fill = "#7463AC", color = "white",
                    binwidth = 1) +
     theme_minimal() +
     labs(x = "Square root", y = "")
```

-   Finally, we plot the graphs using gridExtra so that we can see all 4.

```{r, fig.alt="Transformations as Histograms of PHYSHLTH Variable"}
gridExtra::grid.arrange(cuberoot, logged, inversed, squareroot)
```

-   In this example, NOT ONE transformation helped. If this happens, something else would need to occur before correctly using the variable. Examples could be to run a non-linear model, or categorizing the data into bins, especially since there was a large frequency of people that were not ill.

# Introduction to RMarkdown

-   R Markdown (RMD) files are a versatile format that combines plain text, code, and output to create dynamic and reproducible documents.
-   Benefits of Using RMD Files:
    -   Reproducibility: Combining code and output ensures that the document is reproducible. Any changes in the data or analysis can be quickly updated.
    -   Integration: RMD files seamlessly integrate with R, allowing you to run and display results from your analysis within the same document.
    -   Flexibility: You can produce various types of reports, including static documents and dynamic documents.
    -   Collaboration: Being that RMD files are plain text, they are easy to share and version control with tools like Git.
-   RMD files have many options, but generally are composed of the following:
    -   yaml Header
    -   markdown Text
    -   Code Chunks
    -   Output Formats
    -   Visualization and Plots

## yaml Header

-   The YAML header is located at the beginning of the RMD file and contains metadata about the document, such as the title, author, date, and output format. Here is an example of a YAML header:

![YAML Header Example](Pictures/Ch0/ymlHeader.png "YAML Header Example")

## markdown Text

-   Markdown is a lightweight markup language with plain text formatting syntax. In an RMD file, you use Markdown to format the text, create lists, headers, links, and more. For example:

![Styles Example](Pictures/RMD/markdownStyles.png "Styles")

## Code Chunks

-   Code chunks allow you to embed R (or other languages) code within the document. These chunks are executed when the document is rendered, and their output is included in the final document. Code chunks are defined with triple backticks and the language, like this:

![Code Chunk Example](Pictures/RMD/codechunk.png "Code Chunk")

-   Chunk output can be customized with knitr options, arguments set in the {} of a chunk header.
    -   include = FALSE prevents code and results from appearing in the finished file. R Markdown still runs the code in the chunk, and the results can be used by other chunks.
    -   echo = FALSE prevents code, but not the results from appearing in the finished file. This is a useful way to embed figures.
    -   message = FALSE prevents messages that are generated by code from appearing in the finished file.
    -   warning = FALSE prevents warnings that are generated by code from appearing in the finished.
    -   fig.cap = "..." adds a caption to graphical results.

![Arguments of Chunk](Pictures/RMD/message.png "Arguments of Chunk")

## Output Formats

-   RMD files can be rendered into various output formats including HTML, PDF, and Word documents. The output format is specified in the YAML header. To generate the final document, you use the knit function in RStudio or call rmarkdown::render() in your R script.

## Visualization and Plots

-   RMD files can include visualizations and plots. For example, you can create and embed a plot using the ggplot2 package.

library(ggplot2) ggplot(mpg, aes(x=displ, y=hwy)) + geom_point()

```{r}

library(ggplot2)
ggplot(mpg, aes(x=displ, y=hwy)) + geom_point()

```

## Packages Required for RMD to Work

-   RMarkdown requires updated packages for formatting and when saving as .pdf documents, a latex extension is also required so that it properly reads the formulas in the problems. Download the .R file on Blackboard and follow the instructions provided to use RMarkdown.
-   The R file includes the following commands:

```{r, eval=FALSE}
install.packages("rmarkdown")
install.packages("knitr")
install.packages("formatR")
tinytex::install_tinytex()  # Select Y when/if it asks down in the console.
```

-   To use RMarkdown most efficiently, make sure your version of R is the latest by checking the website. If it is not, then take the time to get the latest version of the software. Second, make sure your files are not on the cloud, including OneDrive. If so, then decouple at least your desktop with OneDrive so you can work off the cloud. Finally, run the lines below one at a time. Once the packages are done installing, then restart your computer. Finally, open RStudio and run a blank RMarkdown file. If this file knits, then your homework should knit as long as there are no other errors.
-   You may only need to update one of these packages. However, since they are all connected, running these commands ensures that they are all up to date with minimal troubleshooting.

## Troubleshooting RMD FIles

-   Lack of Syntax Errors:

    -   R Markdown files may fail to knit without providing clear syntax error messages.
    -   To troubleshoot, run each code chunk interactively in RStudio to identify and fix errors before knitting.
    -   Pay attention to any warnings or messages in the console as they may indicate potential issues.

-   Using Relative Links to Datasets:

    -   If the .Rmd file relies on external datasets, ensure that file paths are specified using relative links.
    -   Relative links ensure portability, allowing the .Rmd file to work on different systems without modification.
    -   For example, if a dataset is in a subfolder, use a path like data \<- read.csv("data/my_dataset.csv").
    -   Always verify that the dataset exists in the specified location relative to the working directory.

-   Including Necessary Libraries:

    -   Functions from external libraries will fail if the required libraries are not loaded.
    -   At the beginning of the .Rmd file, load all necessary libraries explicitly using library() calls, e.g., library(tidyverse).
    -   Ensure that all libraries used in the document are installed beforehand by running install.packages() if needed.
    -   Loading libraries early prevents errors during knitting and avoids confusing error messages.

## Full Example Using nhanes Dataset

-   A big example of where charts are helpful is to explore the nhanes dataset, which is actually a dataset related to auditory issues, in which the number of guns fired is a variable.
-   Specifically, AUQ060 refers to "Hear a whisper from across a quiet room?" AUQ070 refers to "Hear normal voice across a quiet room?" and AUQ080 refers to "Hear a shout from across a quiet room?" While AUQ300 refers to "Ever used firearms for any reason? AUQ310 refers to "How many total rounds ever fired?" and AUQ320 refers to "Wear hearing protection when shooting?" I got these references at the following website: <https://wwwn.cdc.gov/Nchs/Nhanes/2011-2012/AUQ_G.htm>.
-   It would be interesting to determine whether there is a relationship between hearing loss and gun use. Below, I clean the data set and make some charts. All variables are categorical, and I only look at 2 categorical variables at a time - one auditory related (AUQ 060, 070, 080) and one gun related (AUQ 300, 310, 320).
-   The first step is data cleaning, and in particular recoding the variables of interest. We did some of these before. Let's also filter out the unused variables using the select statement.\
-   The select statement in dplyr has a conflict with the select statement in MASS. Since we used MASS earlier, we have to specify which package we want to use select from. We want to use select from dplyr, so we add dplyr:: before the function.

```{r, warning=FALSE}
nhanes <- read.csv("data/nhanes2012.csv")
#summary(nhanes)

nhanes.clean <- nhanes %>%
  dplyr::select(AUQ300,AUQ310, AUQ320, AUQ060, AUQ070, AUQ080) %>%
  mutate(AUQ300 = recode_factor(AUQ300,
           '1' = 'Yes',
           '2' = 'No')) %>%
  mutate(AUQ310 = recode_factor(AUQ310,
           '1' = "1 to less than 100",
           '2' = "100 to less than 1000",
           '3' = "1000 to less than 10k",
           '4' = "10k to less than 50k",
           '5' = "50k or more",
           '7' = "Refused",
           '9' = "Don't know")) %>%
  mutate(AUQ060 = recode_factor(AUQ060,
           '1' = 'Yes',
           '2' = 'No'))%>%
  mutate(AUQ070 = recode_factor(AUQ070,
           '1' = 'Yes',
           '2' = 'No'))%>%
  mutate(AUQ080 = recode_factor(AUQ080,
           '1' = 'Yes',
           '2' = 'No'))%>%
  mutate(AUQ320 = recode_factor(AUQ320,
           '1' = 'Always',
           '2' = 'Usually',
           '3' = 'About half the time',
           '4' = 'Seldom',
           '5' = 'Never'))
summary(nhanes.clean)

```

-   From here, NA's are an issue, but we don't want to broadly omit because it would slice down our dataset too much. I am going to leave them in and handle it on a chart by chart basis.
-   The most applicable chart to graph 2 categorical variables is a barchart. This requires calculating frequencies, and then graphing. Using ggplot, the frequencies are calculated automatically.

```{r}

nhanes.clean %>% 
  drop_na(AUQ310)%>% 
  drop_na(AUQ060)%>% 
  ggplot(aes(x=AUQ310, fill=AUQ060)) + geom_bar(position="dodge") + 
  labs(x="How many rounds have you fired", title="Hearing Whisper Across Room vs. Num Rounds Fired", y="Frequency")

nhanes.clean %>% 
  drop_na(AUQ310)%>% 
  drop_na(AUQ070)%>% 
  ggplot(aes(x=AUQ310, fill=AUQ070)) + geom_bar(position="dodge") + 
  labs(x="How many rounds have you fired", title="Hearing Normal Across Room vs. Num Rounds Fired", y="Frequency")


nhanes.clean %>% 
  drop_na(AUQ310)%>% 
  drop_na(AUQ080)%>% 
  ggplot(aes(x=AUQ310, fill=AUQ080)) + geom_bar(position="dodge") + 
  labs(x="How many rounds have you fired", title="Hearing Shout Across Room vs. Num Rounds Fired", y="Frequency")

#############Wear hearing protection when shooting
nhanes.clean %>% 
  drop_na(AUQ320)%>% 
  drop_na(AUQ060)%>% 
  ggplot(aes(x=AUQ320, fill=AUQ060)) + geom_bar(position="dodge") + 
  labs(x="Wear hearing protection when shooting", title="Hearing Whisper Across Room vs. Use of Hearing Protection", y="Frequency")

nhanes.clean %>% 
  drop_na(AUQ320)%>% 
  drop_na(AUQ070)%>% 
  ggplot(aes(x=AUQ320, fill=AUQ070)) + geom_bar(position="dodge") + 
  labs(x="Wear hearing protection when shooting", title="Hearing Normal Across Room vs. Use of Hearing Protection", y="Frequency")


nhanes.clean %>% 
  drop_na(AUQ310)%>% 
  drop_na(AUQ080)%>% 
  ggplot(aes(x=AUQ310, fill=AUQ080)) + geom_bar(position="dodge") + 
  labs(x="Wear hearing protection when shooting", title="Hearing Shout Across Room vs. Use of Hearing Protection", y="Frequency")
```

-   That is a few of the charts. See if you can determine anything interesting from them and also try to run the other combinations. You will see that it is easier to see with less categories when you have multiple variables like this.

# Detecting Outliers

-   We see an outlier visually, but without the tool available, we can detect them through the statistics. First we calculate the IQR, which is just quarter 3 minus quarter 1.

```{r}
summary(GrowthFund$GrowthFund)
IQRvalue <- 16.9425 - 3.875; IQRvalue
IQRvalue <- IQR(GrowthFund$GrowthFund); IQRvalue
```

-   Then, multiply the IQR by 1.5.

```{r}
OutlierValue <- IQRvalue*1.5; OutlierValue
```

-   Finally conduct 2 checks to determine if outliers are past the low whisker and/or high whisker.

    -   A TRUE value indicates that at least one outlier is present at the small end of the distribution.
    -   A FALSE value indicates that no outliers are at the high end of the distribution.

```{r}
QuanData
QuanData[2]-QuanData[1] > OutlierValue 
#True indicating an outlier to the left
3.8750 - -38.32 #42.195
42.195 > 19.60125 #TRUE

QuanData[5]-QuanData[4] > OutlierValue 
#False indicating no outlier to the right. 
36.29-16.9425  #19.3475
19.3475 > 19.60125 #FALSE 
```

-   You can also more formally test by using the following formulas.
    -   $Lower bound=Q1−1.5×IQR$
    -   $Upper bound=Q3+1.5×IQR$
    -   A data point $x$ is an outlier if $x$ less than lower bound or $x$ is greater than the upper bound. Confirming what we found above, we determine one outlier is present to the left.

```{r}
LowerBound <- 3.8750  - 1.5* IQRvalue; LowerBound
Q1 <- QuanData[2]; Q1 #3.875
LowerBound <- Q1 - OutlierValue; LowerBound #-15.72625

UpperBound <- 16.9425 + 1.5 * IQRvalue; UpperBound
Q3 <- QuanData[4]; Q3 #16.9425
UpperBound <- Q3+OutlierValue; UpperBound #36.54375

##Insert Lower bound and Upper bound in vector to determine if outliers are present: (-38.32, LowerBound 1.71, 3.17, 5.99, 12.56, 13.47, 16.89, 16.96, 32.16, 36.29, UpperBound)

##one outlier to the left, -38.32. 
```

-   Let's get some summary statistics to check for outliers, skewness, and kurtosis to see how our visual aids help us in understanding those results.

```{r}
IQRvalue <- IQR(mtcars$mpg)
OutlierValue <- IQRvalue*1.5; OutlierValue #11.0625
QuanData <- quantile(mtcars$mpg); QuanData

QuanData[2]-QuanData[1] > OutlierValue 
# Using the numbers from QuadData
15.425  - 10.400 >11.0625
#False indicating no outlier to the left


QuanData[5]-QuanData[4] > OutlierValue 
33.900 - 22.800 > 11.0625
#TRUE indicating an outlier to the right. 
```

-   We can see more specific information on outliers if we calculate the lower bound and upper bound and insert the values into the vector.

```{r}
Q1 <- QuanData[2]; Q1
LowerBound <- Q1 - OutlierValue; LowerBound #4.3625 

Q3 <- QuanData[4]; Q3 
UpperBound <- Q3+OutlierValue; UpperBound #33.8625 

sort(mtcars$mpg)
```

$$LowerBound: 4.3625$$ 10.4 10.4 13.3 14.3 14.7 15.0 15.2 15.2 15.5 15.8 16.4 17.3 17.8 18.1 18.7 19.2 19.2 19.7 21.0 21.0 21.4 21.4 21.5 22.8 22.8 24.4 26.0 27.3 30.4 30.4 32.4 $$UpperBound: 33.8625)$$ 33.9

# Summarize Data

-   In R, summary() and summarize() serve different purposes. summary() is part of base R and gives a quick overview of data, returning descriptive statistics for each column. For example, summary(mtcars) provides the min, max, median, and mean for numeric columns and counts for factors. It’s useful for a broad snapshot of your dataset.

-   In contrast, summarize() (or summarise()) is from the dplyr package and allows for custom summaries. For instance, mtcars %\>% summarize(avg_mpg = mean(mpg), max_hp = max(hp)) returns the average miles per gallon and the maximum horsepower. It’s more flexible and is often used with group_by() for grouped calculations. In conclusion, summary() gives automatic overviews, while summarize() is better for tailored summaries.

-   Use the summary() function to examine the contents of the file.

```{r}
summary(object = gss.2016) 
```

-   Again, we can eliminate the object = because it is the first argument and is required.

```{r}
summary(gss.2016)
```
