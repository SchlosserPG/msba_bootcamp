---
title: "Data Preparation"
output: html_document
execute: 
  warning: false
  error: false
---



-   The goal of this lesson is to teach you how to clean datasets for use in analytics. This lesson focuses on dplyr. dplyr is a package in R that provides a set of functions for data manipulation tasks. These functions are designed to be intuitive and efficient, making it easier to work with data frames or tibbles (a modern reimagining of data frames provided by the tibble package).


### At a Glance

-   In order to succeed in this lesson, we need to be able to evaluate variables and understand how to clean and prepare data to make variables easier to use and in the correct form. This sometimes includes subsetting and filtering data alongside other techniques.

### Lesson Objectives

-   Create variables and identify and change data types.
-   Learn how to clean data via dplyr.

### Consider While Reading

-   We often spend a considerable amount of time inspecting and preparing the data for the subsequent analysis. This includes the following:
    -   Sorting Data
    -   Selecting Variables
    -   Filtering Data
    -   Counting Data
    -   Handling Missing Values
    -   Summarizing
    -   Grouping Data

# case_when

-   The case_when() function from the dplyr package is used to create new variables based on multiple conditions. It works like a series of if...else if...else statements and is especially useful for assigning values based on ranges or logical categories.
-   The case_when() and recode() functions in R both allow you to transform or recode values in a vector, but they differ in flexibility and use cases. recode() is best for straightforward value replacement, where each input value maps directly to a new value—such as turning "good" into 3.
-   In contrast, case_when() is more flexible and powerful, allowing you to specify complex conditional logic, including comparisons and ranges (e.g., x \> 100 \~ "High"). case_when() evaluates conditions in order and is particularly useful when the transformation depends on logical expressions rather than exact matching. For categorical data that maps cleanly one-to-one, recode() is convenient; for anything requiring conditional logic, case_when() is preferred.

```{r, echo=FALSE, include=TRUE}
#data <- data %>%
#  mutate(NewVariable = case_when(
#    condition1 ~ value1,
#    condition2 ~ value2,
#    TRUE ~ default_value  # optional default
```

-   From recoding to ordinal Variables

```{r}
library(tidyverse)
evaluate <- c("excellent", "good", "fair", "poor", "excellent", "good")
data <- data.frame(evaluate)

dataRecode <- data %>%
     mutate(evaluate = recode(evaluate,
            "excellent" = 4,
            "good" = 3,
            "fair" = 2,
            "poor" = 1))
```

-   If we alter this to a case_when, we would include the following.

```{r}
dataCase_When <- data %>%
  mutate(evaluate = case_when(
    evaluate == "excellent" ~ 4,
    evaluate == "good" ~ 3,
    evaluate == "fair" ~ 2,
    evaluate == "poor" ~ 1,
    TRUE ~ NA_real_  # for safety in case of unexpected values
  ))
dataCase_When
```

-   If we were to give a range, we could do that with case_when using the \>, \< or \>= or \<= signs. An example is below.

```{r}
# Sample vector of numbers
score <- c(9, 6, 3, 8, 5, 10, 2)

# Categorize using case_when
category <- case_when(
  score > 8        ~ "High",
  score >= 5       ~ "Medium",
  score < 5        ~ "Low"
)

# Combine into a data frame to view
df <- data.frame(score, category); df

```

-   This function is useful in working with dates.

## Date data type

-   The date data type in R is used to represent calendar dates, allowing for accurate storage and manipulation of time-related data. Dates in R are typically stored as objects of class Date, which internally represent the number of days since January 1, 1970. This format facilitates arithmetic operations, such as calculating differences between dates or adding/subtracting days. Dates can be created and manipulated using functions like as.Date() for converting character strings to date objects, or Sys.Date() for retrieving the current date. Handling dates correctly is essential in time series analysis, scheduling, and any task involving chronological data, as improper formatting or assumptions can lead to errors in analysis.

```{r}
# Convert date info in format 'mm/dd/yyyy' using as.Date
strDates <- c("01/05/1965", "08/16/1975")
dates <- as.Date(strDates, "%m/%d/%Y") 
str(dates)
```

-   The lubridate package provides additional tools for working with dates, such as parsing and extracting components like year, month, and day. This package makes dates a lot easier to work with.

```{r}
# Convert date info in format 'mm/dd/yyyy' using lubridate
library(lubridate)
strDates <- c("01/05/1965", "08/16/1975")
dates <- mdy(strDates) 
str(dates)
```

-   If you are only given a year and a month, you can use the ym() command to turn it to a date. But take note that it will add a day to the value as a placeholder.

```{r}
# Convert date info in format 'yyyymm' using lubridate
stryyyymm <- c("202201", "202003", "202204")
dates <- ym(stryyyymm)
str(dates)
```

```{r}
# Convert date info in format 'mm/dd/yyyy' using as.Date
strDates <- c("01/05/1965", "08/16/1975")
dates <- as.Date(strDates, "%m/%d/%Y") 
str(dates)
```

```{r}
UseDates <- read.csv("data/Lubridate.csv")
UseDates$Date <- ym(UseDates$Date)

#You can use the month or year function to access the month or year from the date
UseDates$month <- month(UseDates$Date)
UseDates$year <- year(UseDates$Date)

##Now, you can access months or year numerically
summary(UseDates$month) 
summary(UseDates$year)

##You could code the seasons using a mutate function making a new variable "season"
library(tidyverse)
UseDates <- UseDates %>% 
  mutate(season = case_when(
    month %in% 3:5 ~ "Spring",
    month %in% 6:8 ~ "Summer",
    month %in% 9:11 ~ "Fall",
    TRUE ~ "Winter"
  ))

UseDates$season <- as.factor(UseDates$season)
summary(UseDates$season)

##You could use a filter statement to select a particular year. 
#In the example below, I save the filtered data into a new data frame UseDate2022. 
UseDate2022 <- filter(UseDates, year==2022)

```

-   Below lists some common commands using Lubridate.

$$
\begin{array}{|l|l|}
\hline
\textbf{Function} & \textbf{Description} \\
\hline
\texttt{ymd("2025-07-08")} & \text{Parse a date in Year-Month-Day format} \\
\texttt{mdy("July 8, 2025")} & \text{Parse a date in Month-Day-Year format} \\
\texttt{ym("2025-07")} & \text{Parse Year-Month (no day)} \\
\texttt{today()} & \text{Get the current date} \\
\texttt{now()} & \text{Get the current date and time} \\
\texttt{year(date)} & \text{Extract the year from a date} \\
\texttt{month(date)} & \text{Extract the month (number)} \\
\texttt{month(date, label=TRUE)} & \text{Extract the month (name)} \\
\texttt{day(date)} & \text{Extract the day of the month} \\
\texttt{wday(date)} & \text{Get weekday (number)} \\
\texttt{wday(date, label=TRUE)} & \text{Get weekday (name)} \\
\hline
\end{array}
$$

# Common dplyr Functions

### Arrange

-   Sorting or arranging the dataset allows you to specify an order based on variable values.
-   Sorting allows us to review the range of values for each variable, and we can sort based on a single or multiple variables.
-   Notice the difference between sort() and arrange() functions below.
    -   The sort() function sorts a vector.
    -   The arrange() function sorts a dataset based on a variable.
-   To conduct an example, read in the data set called gig.csv from your working directory.

```{r}
gig <- read.csv("data/gig.csv", stringsAsFactors = TRUE, na.strings="")
dim(gig)
head(gig)
```

-   Using the arrange() function, we add the dataset, followed by a comma and then add in the variable we want to sort. This arranges from small to large.

-   Below is code to rearrange data based on Wage and save it in a new object.

```{r}
sortTidy <- arrange(gig, Wage)
head(sortTidy)
```

-   We can apply a desc() function inside the arrange function to re-sort from high to low like shown below.

```{r}
sortTidyDesc <- arrange(gig, desc(Wage))
head(sortTidyDesc)
```

### Subsetting or Filtering

-   Subsetting or filtering a data frame is the process of indexing, or extracting a portion of the data set that is relevant for subsequent statistical analysis. Subsetting allows you to work with a subset of your data, which is essential for data analysis and manipulation. One of the most common ways to subset in R is by using square brackets \[\]. We can also use the filter() function from tidyverse.

-   We use subsets to do the following:

    -   View data based on specific data values or ranges.
    -   Compare two or more subsets of the data.
    -   Eliminate observations that contain missing values, low-quality data, or outliers.
    -   Exclude variables that contain redundant information, or variables with excessive amounts of missing values.

-   When working with data frames, you can subset by rows and columns using two indices inside the square brackets: data\[row, column\]. For example, if you have df \<- data.frame(a = 1:3, b = c("X", "Y", "Z")), df\[1, 2\] would return the value "X", which is the first row and second column. If you want the entire first row, you would use df\[1, \], and to get the second column, you’d use df\[, 2\].

-   You can also use logical conditions to subset. For instance, x\[x \> 20\] would return all values in x greater than 20, and in a data frame, you could filter rows where a certain condition holds, such as df\[df\$a \> 1, \], which returns rows where column a has values greater than 1.

-   Let's do an example using the customers.csv file we read in earlier as customers in the last lesson. Base R provides several methods for subsetting data structures. Below uses base R by using the square brackets dataset\[row, column\] format.

```{r, results='hide'}
customers <- read.csv("data/customers.csv", stringsAsFactors = TRUE)

#To subset, note the dataset[row,column] format
#Results hidden to save space, but be sure to try this code in your .R file. 
#Data in 1st row
customers[1,] 
#Data in 2nd column
customers[,2] 
#Data for 2nd column/1st observation (row)
customers[1,2] 
#First 3 columns of data
customers[,1:3] 
```

-   One of the most powerful and intuitive ways to subset data frames in R is by using the filter() function from the dplyr package, which is part of the tidyverse. Tidyverse is extremely popular when filtering data.
-   The filter function is used to subset rows of a data frame based on certain conditions.
-   The below example filters data by the College variable when category values are "Yes" and saves the filtered dataset into an object called college.

```{r}
#Filtering by whether the customer has a "Yes" for college. 
#Saving this filter into a new object college which you should see in your global environment. 
college <- filter(customers, College == "Yes")
#Showing first 6 records of college - note the College variable is all Yes's. 
head(college)
```

-   Using the filter command, we can add filters pretty easily by using an & for and, or an \| for or. The statement below filters by College *and* Income and save the new dataset in an object called twoFilters.

```{r}
twoFilters <- filter(customers, College == "Yes" & Income < 50000)
head(twoFilters)
```

-   Next, we can do an *or* statement. The example below uses the filter command to filter by more than one category in the same field using the \| in between the categories.

```{r}
TwoRaces <- filter(customers, Race == "Black" | Race == "White")
head(TwoRaces)
```

-   The str_detect() function is used to detect the presence or absence of a pattern (regular expression) in a string or vector of strings. It returns a logical vector indicating whether the pattern was found in each element of the input vector.
-   Using str_detect it with a filter function allows you to pull observations based on the inclusion of a string pattern.

```{r}
library(tidyverse)
Birthday2000 <- filter(customers, str_detect(BirthDate, "1985"))
```

### Select

-   In R, the select() function is part of the dplyr package, which is used for data manipulation. The select() function is specifically designed to subset or choose specific columns from a data frame. It allows you to select variables (columns) by their names or indices.
-   Both statements below select Income, Spending, and Orders variables from the customers dataset and form them into a new dataset called smallData.
-   The statements are written with and without the chaining operator.

```{r}
smallData <- select(customers, Income, Spending, Orders)
head(smallData)
```

### Piping (Chaining) Operator

-   The pipe operator takes the output of the expression on its left-hand side and passes it as the first argument to the function on its right-hand side. This enables you to chain multiple functions together, making the code easier to understand and debug.
-   If we want to keep our code tidy, we can add the piping operator (%\>%) to help combine our lines of code into a new object or overwriting the same object.
-   This operator allows us to pass the result of one function/argument to the other one in sequence.
-   The below example uses a select function to pull Income, Spending, and Orders variables fromt he customers dataset and save it as a new object called smallData. It is an identical request to the one directly above, but written with the piping operator.

```{r}
smallData <- customers %>% select(Income, Spending, Orders)
```

### Counting

-   Counting allows us to gain a better understanding and insights into the data.

-   This helps to verify that the data set is complete or determine if there are missing values.

-   In R, the length() function returns the number of elements in a vector, list, or any other object with a length attribute. It essentially counts the number of elements in the specified object.

```{r}
#Gives the length of Industry
length(gig$Industry)
```

-   For counting using tidyverse, we typically use the filter and count function together to filter by a value or state and then count the filtered data.
-   In the function below, I use the piping operator to link together the filter and count functions into one command.
-   Note that we need a piping operator (%\>%) before each new function that is part of the chunk.

```{r}
# Counting with a Categorical Variable
# Here we are filtering by Automotive Industry and then counting the number and saving it in a new object called countAuto
countAuto <- gig %>%
     filter(Industry=="Automotive") %>%
     count()
countAuto #190
```

-   Below, we are filtering by Wage and the counting.

```{r}
# Counting with a Numerical Variable
# We could also save this in an object. 
gig %>%
  filter(Wage > 30) %>%
  count() ##536
```

-   We learned that there are 190 employees in the automotive industry and there are 536 employees who earn more than \$30 per hour.

-   We could also calculate the number of people with wages under or equal to 30.

```{r}
#We find 68 Wages under or equal to 30
WageLess30 <- gig %>%
  filter(Wage <= 30) %>%
  count() #
WageLess30
```

-   How many Accountants are there in the Job Category of the gig data set. The answer is shown below. Use filter and count to calculate this answer.

```{r, echo=FALSE}
gig %>%
     filter(Job=="Accountant") %>%
     count()
## 83 Accountants
```

### Handling Missing Data

-   Missing data is a common issue in data analysis and can arise for various reasons, such as data collection errors, non-responses in surveys, or data corruption. In R, handling missing data is crucial to ensure accurate and reliable analysis. Missing values are typically represented by NA (Not Available) in R.

-   Missing data needs to be closely evaluated and verified within each variable whether the data is truly blank, has no answer, or is marked with a character value such as the text N/A.

-   Missing data needs to be closely evaluated to see if the missing value is meaningful or not. If the variable that has many missing values is deemed unimportant or can be represented using a proxy variable that does not have missing values, the variable may be excluded from the analysis.

-   After a data set is loaded, there are three common strategies for dealing with missing values.

1.  The omission strategy recommends that observations with missing values be excluded from subsequent analysis.

2.  The imputation strategy recommends that the missing values be replaced with some reasonable imputed values. For example, imputing missing values using techniques like mean/median substitution or regression models can be considered.

    -   Numeric variables: replace with the average.
    -   Categorical variables: replace with the predominant category.

3.  Ignore your missing data if the function works without it.

    -   When you ignore missing data because your function works without it, the missing values are typically excluded from the calculations by default. In R, many functions, such as mean(), sum(), or lm(), have arguments like na.rm = TRUE to explicitly remove missing values during computation. Ignoring missing data can simplify the analysis, but it comes with potential consequences.

-   The choice of approach depends on the nature of the missingness, which can be categorized as Missing Completely at Random, Missing at Random, or Missing Not at Random. Addressing missing data appropriately is essential to maintain the validity of statistical analyses and avoid biases.

#### Limitations of Using a Missing Data Technique

-   Recommended Closer Evaluation of Missing Data

-   There are limitations of the techniques listed above (omission, imputation, and ignore).

-   Reduction in Sample Size: Ignoring missing data leads to a smaller effective sample size, which may reduce the power of your analysis and the reliability of the results.

-   Bias: If the missing data are not Missing Completely at Random, ignoring them may introduce bias. For example, if specific groups or patterns are overrepresented in the remaining data, the results may not generalize to the full dataset.

-   Distorted Metrics: Calculations that ignore missing values (e.g., averages, sums) might not reflect the true population parameters, especially if the missing data are systematically different from the observed data. In addition, if a large number of values are missing, mean imputation will likely distort the relationships among variables, leading to biased results.

-   Incorrect Inferences: Ignoring missing data without considering its nature could lead to incorrect conclusions, as the analysis only reflects the subset of available data.

-   Consider a dataset used to predict factors that lead to intubation due to COVID-19. Suppose one variable, "Number of pregnancies," contains missing data (NAs) for all men, as the question is not applicable to them. If we were to compare this variable with another, "Intubated due to COVID-19: Yes/No," simply omitting the rows with blanks (NAs) could lead to the exclusion of an entire gender, distorting the analysis. In this case, a different approach to handling missing data would be more appropriate to ensure the dataset remains representative. Additionally, if a value is not blank but is considered missing for analysis purposes, the data should be consistently processed (e.g., mutated or recoded) to align with the chosen technique for handling true missing values.

#### na.rm

-   The na.rm parameter in R is a convenient way to handle missing values (NA) within functions that perform calculations on datasets. The parameter stands for "NA remove" and, when set to TRUE, instructs the function to exclude missing values from the computation.

```{r}
y <- c(1, 2, NA, 3, 4, NA)
# These lines runs, but do not give you anything useful.
sum(y) 
mean(y)
```

-   Many functions in R include parameters that will ignore NAs for you.

-   sum() and mean() are examples of this, and most summary statistics like median() and var() and max() also use the na.rm parameter to ignore the NAs. Always check the help to determine if na.rm is a parameter.

```{r}
sum(y, na.rm=TRUE) 
mean(y, na.rm=TRUE)
# na.omit removes the NAs from the vector of dataset. Here, it works for removing NAs from the vector.  
y <- na.omit(y) 
```

#### na.rm with a a dataset

-   Using a dataset, we need the \_data$variable_ format, like mean(data$column, na.rm = TRUE) calculates the mean of the non-missing values in the specified column or variable. This approach is straightforward and useful when the presence of missing values would otherwise cause an error or return an NA result.

```{r}
gig <- read.csv("data/gig.csv", stringsAsFactors = TRUE, na.strings="")
summary(gig)
mean(gig$Wage, na.rm=TRUE)
```

#### is.na()

-   In R, the is.na() function is used to check for missing (NA) values in objects like vectors, data frames, or arrays. It returns a logical vector of the same length as the input object, where TRUE indicates a missing value and FALSE indicates a non-missing value.

```{r}
#Counts the number of all NA values in the entire dataset
CountAllBlanks <- sum(is.na(gig)); CountAllBlanks 

#Gives the observation number of the observations that include NA values
which(is.na(gig$Industry))

#Produces a dataset with observations that have NA values in the Industry field. 
ShowBlankObservations <- gig %>%
     filter(is.na(Industry))
ShowBlankObservations

#Counts the number of observations that have NA values in the Industry field. Industry is categorical, so we can count values based on it. 
CountBlanks <- sum(is.na(gig$Industry)); CountBlanks 

library(tidyverse)
#Counts the number of observations that have NA values in the Wage field. 
CountBlanks <- sum(is.na(gig$Wage)); CountBlanks 

```

#### na_if()

-   The na_if() function in tidyr is used to replace specific values in a column with NA (missing) values. This function can be particularly useful when you want to standardize missing values across a dataset or when you want to replace certain values with NA for further data processing

```{r}
TurnNA <- gig %>%
     mutate(Job = na_if(Job, "Other"))
head(TurnNA)
```

#### na.omit() vs. drop_na()

-   Both functions return a new object with the rows containing missing values removed.

-   na.omit() is a base R function, so it doesn't require any additional package installation where drop_na() requires loading the tidyr package, which is part of the tidyverse ecosystem.

-   drop_na() fits well into tidyverse pipelines, making it easy to integrate with other tidyverse functions where na.omit() can also be used in pipelines but might require additional steps to fit seamlessly.

```{r}
#install.packages("Amelia")
library(Amelia)
data("africa")
summary(africa)
summary(africa$gdp_pc)
summary(africa$trade)

africa1 <- na.omit(africa)
summary(africa1)

##to drop all at once. 
africa2 <- africa %>% drop_na()
summary(africa2)

```

-   You try to load the airquality dataset from base R and look at a summary of the dataset.

    -   Sum the number of NAs in airquality.
    -   Omit all the NAs from airquality and save it in a new data object called airqual and take a new summary of it.

    ```{r, echo=FALSE}
    data("airquality")
    summary(airquality)
    sum(is.na(airquality))
    #37 + 7
    airqual <- na.omit(airquality)
    #153-111
    summary(airqual)
    ```

### Summarize

-   The summarize() command is used to create summary statistics for groups of observations in a data frame.
-   In R, summary() and summarize() serve different purposes. summary() is part of base R and gives a quick overview of data, returning descriptive statistics for each column. For example, summary(mtcars) provides the min, max, median, and mean for numeric columns and counts for factors. It’s useful for a broad snapshot of your dataset.
-   In contrast, summarize() (or summarise()) is from the dplyr package and allows for custom summaries. For instance, mtcars %\>% summarize(avg_mpg = mean(mpg), max_hp = max(hp)) returns the average miles per gallon and the maximum horsepower. It’s more flexible and is often used with group_by() for grouped calculations. In conclusion, summary() gives automatic overviews, while summarize() is better for tailored summaries.
-   In the example below, we can summarize more than one thing into tidy output.

```{r}
gig %>%
     drop_na() %>% 
     summarize(mean.days = mean(Wage),
               sd.days = sd(Wage),
               var.days = var(Wage),
               med.days = median(Wage),
               iqr.days = IQR(Wage))

```

### Group_by

-   group_by is used for grouping data by one or more variables. When you use group_by() on a data frame, it doesn't actually perform any computations immediately. Instead, it sets up the data frame in such a way that any subsequent operations are performed within these groups
-   summarize() is often used in combination with group_by() to calculate summary statistics within groups

```{r}
##summarize data by Industry variable. 
groupedData <- gig %>%
     group_by(Industry) %>%
     summarize(meanWage = mean(Wage))
groupedData

##same function with na's dropped. 
groupedData <- gig %>%
     drop_na() %>%
     group_by(Industry) %>%
     summarize(meanWage = mean(Wage))
groupedData

```

### Mutate

-   mutate() is part of the dplyr package, which is used for data manipulation. The mutate() function is specifically designed to create new variables (columns) or modify existing variables in a data frame. It is commonly used in data wrangling tasks to add calculated columns or transform existing ones.
-   One example is below, but note that there are many things you can do with the mutate function.

```{r}
#making a new variable called calculation that multiplies gdp_pc by infl variables in the africa1 dataset. 
africa.mutated <- mutate(africa1, calculation = gdp_pc * infl)
head(africa.mutated)
```

-   Below is an example with the iris dataset, which is part of base R.

```{r}
data("iris")
##Selecting 2 variables from the iris dataset: Sepal.Length and Petal.Length
selected_data <-  select(iris, Sepal.Length, Petal.Length)
head(selected_data)
# Filter rows based on a condition: Species = setosa
filtered_data <-  filter(iris, Species == "setosa")
head(filtered_data)
# Arrange rows by the Sepal.Length column
arranged_data <-  arrange(iris, Sepal.Length)
# Create a new column by mutating the data by transforming Petal.Width to the log form. 
mutated_data <- mutate(iris, Petal.Width_Log = log(Petal.Width))
```

## Full Examples

### gss.2016 Data Cleaning

-   First, because we made some edits to the data set, reread in version a using the read.csv command. This brings the data set back to its original form. It is always a good idea to read the dataset back in when you are unsure about whether you have made a mistake during data preparation that could cause a lack of data integrity.\

```{r message=FALSE}
gss.2016 <- read.csv(file = "data/gss2016.csv") 
```

-   Before we remove any missing data, we need it to be the correct data type. In this case, grass should be a factor.

```{r}
# We coerced this variable earlier, but the object was called gss.2016. 
#Since we reread in the data set, this needs to be done again. 
gss.2016$grass <- as.factor(gss.2016$grass)
```

-   The statement below is an equivalent to the function above, but written with the piping operator. It is overwriting gss.2016 after conducting the coercion to factor.
-   We added the mutate function because we are going to add other data cleaning tasks to this statement.

```{r}
gss.2016 <- gss.2016 %>% mutate(grass = as.factor(grass))
```

#### Piping to More Functions: Missing Data

-   In the code below, the as.factor() command has been moved inside a broader mutate statement (that uses tidyverse library) and piped to it the na_if() command that handles missing data. If you use more than one data manipulation statement, the mutate() command is needed to help organize your code with one mutate() is needed for each major change you are making.
-   In the code below, we created a new object gss.2016.cleaned to help store the cleaned version of the dataset. This helps maintain data integrity because your original dataset is still intact and each time, you rerun the entire chunk, which includes all the changes at one time.

```{r}
gss.2016.cleaned <- gss.2016 %>%
  #Moved coercion statement into a mutate function to keep code tidy
  mutate(grass = as.factor(grass)) %>% 
  #Moving DK value to NA for not applicable
  mutate(grass = na_if(x = grass, y = "DK"))

#Check the summary, there should be 110 + 3 = 113 in the NA category 
summary(object = gss.2016.cleaned)
```

#### Drop Levels

-   The droplevels function is part of base R and is used to drop unused levels from factor variables in a data frame. It works by removing any levels from a factor variable that are not present in the data.

-   Next, we want to edit our code to convert IAP and DK to NA values and drop levels that have are empty.

    -   Note the Piping operator added to the end of the DK line so you can keep going with new commands editing gss.2016.cleaned.

    ```{r}
    gss.2016.cleaned <- gss.2016 %>% 
      mutate(grass = as.factor(grass)) %>% 
      #Added piping operator
      mutate(grass = na_if(x = grass, y = "DK")) %>%   
      #Turn to na if value of grass = IAP
      mutate(grass = na_if(x = grass, y = "IAP")) %>% 
      #Drop levels in grass that have no values
      mutate(grass = droplevels(x = grass))  
    #Check what you just did
    summary(gss.2016.cleaned) 
    ```

#### Coercing to Numeric

-   Next, we handle a numerical variable, age. Age again has an issue being able to be numerical data type because it has "89 OR OLDER" as a value. Before using the as.numeric() command, we need to recode it. We did this above as a stand-alone statement.\

```{r}
gss.2016.cleaned <- gss.2016 %>% 
  mutate(grass = as.factor(grass)) %>% 
  mutate(grass = na_if(x = grass, y = "DK")) %>% 
  mutate(grass = na_if(x = grass, y = "IAP")) %>% 
  #Added piping operator 
  mutate(grass = droplevels(x = grass)) %>% 
  #Ensure variable can be coded as numeric and fix if necessary. 
  mutate(age = recode(age, "89 OR OLDER" = "89")) %>% 
  #Coerce into numeric
  mutate(age = as.numeric(x = age)) 

#Check what you just did
summary(gss.2016.cleaned) 
```

-   The recode() command that is part of dplyr is like the ifelse() command that is in base R. There are a lot of ways to recode in R.

-   Finally, we want to take our numerical variable, age, and cut it at certain breaks to make categories that can be easily analyzed.

    -   This also ensures that anyone above 89 is coded correctly in a category instead of as the value 89. This again brings back data integrity.
    -   Last, we want to recode the age variable into categories. We can use the case_when to do this.

```{r}
gss.2016.cleaned <- gss.2016 %>% 
     mutate(grass = as.factor(grass)) %>% 
     mutate(grass = na_if(x = grass, y = "DK")) %>% 
     mutate(grass = na_if(x = grass, y = "IAP")) %>% 
     mutate(grass = droplevels(x = grass)) %>% 
     mutate(age = recode(age, "89 OR OLDER" = "89")) %>% 
     #Added piping operator
     mutate(age = as.numeric(x = age)) %>% 
     #Cut numeric variable into groupings
     mutate(age.cat = as.factor(case_when(
          age < 30 ~ "< 30",
          age >= 30 & age <= 59 ~ "30 - 59",
          age >= 60 & age <= 74 ~ "60 - 74",
          age >= 75 ~ "75+",
          TRUE ~ NA_character_  # Safety net for NAs
     )))
#Check what you just did
summary(gss.2016.cleaned) 
```

### brfss Data Cleaning

-   The full codebook where this screenshot is taken is brfss_2014_codebook.pdf.

![Evaluate CodeBook Before Making Decisions](Pictures/Ch2/CodeBookPHYSHLTH.png "Evaluate CodeBook Before Making Decisions")

```{r}
brfss <- read.csv("data/brfss.csv")
summary(brfss)
```

#### Qualitative Variable

-   To look at an example, the one below seeks to understand the healthcare issue in reporting gender based on different definitions. The dataset is part of the Behavioral Risk Factor Surveillance System (brfss) dataset (2014), which includes lots of other variables besides reported gender.

```{r}
#Load the data
brfss <- read.csv("data/brfss.csv")
#Summarize the TRNSGNDR variable
summary(object = brfss$TRNSGNDR) 
#Find frequencies 
table(brfss$TRNSGNDR) 
```

-   Since this table is not very informative, we need to do some edits.
-   Check the class of the variable to see the issue with analyzing it as a categorical variable.

```{r}
class(brfss$TRNSGNDR)
```

-   First, we need to change the TRNSGNDR variable to a factor using as.factor().

```{r}
# Change variable from numeric to factor
brfss$TRNSGNDR <- as.factor(brfss$TRNSGNDR)
# Check data type again to ensure factor
class(brfss$TRNSGNDR)
```

-   Then, we need to do some data cleaning on the TRNSGNDR Variable.

```{r tidy=FALSE}
brfss.cleaned <- brfss %>% 
  mutate(TRNSGNDR = recode_factor(TRNSGNDR,
      '1' = 'Male to female',
      '2' = 'Female to male',
      '3' = 'Gender non-conforming',
      '4' = 'Not transgender',
      '7' = 'Not sure',
      '9' = 'Refused'))
```

-   We can use the levels() command to show the factor levels made with the mutate() command above.

```{r}
levels(brfss.cleaned$TRNSGNDR)
```

-   Check the summary.

```{r}
summary(brfss.cleaned$TRNSGNDR)
```

-   Take a good look at the table to interpret the frequencies in the output above. The highest percentage was the "NA's" category, followed by "Not transgender". Removing the NA's moved the "Not transgender" category to over 97% of observations.

#### Quantitative Variable

-   Let's use the cleaned dataset to make more changes to the continuous variable PHYSHLTH. In the codebook, it looks like the data is most applicable to the first 2 categories. The 1-30 days coding and the 88 coding, which means 0 days of physical illness and injury.
    -   Using cleaned data, we need to prep the variable a little more before getting an accurate plot.
    -   Specifically, we need to null out the 77 and 99 values and make sure the 88 coding is set to be 0 for 0 days of illness and injury.

```{r, fig.alt = "Histogram Generated by R of PHYSHLTH variable"}
brfss.cleaned <- brfss %>% 
  mutate(TRNSGNDR = recode_factor(TRNSGNDR,
      '1' = 'Male to female',
      '2' = 'Female to male',
      '3' = 'Gender non-conforming',
      '4' = 'Not transgender',
      '7' = 'Not sure',
      '9' = 'Refused')) %>%
  #Turn the 77 values to NA's. 
  mutate(PHYSHLTH = na_if(PHYSHLTH, y = 77)) %>%
  #Turn the 99 values to NA's. 
  mutate(PHYSHLTH = na_if(PHYSHLTH, y = 99)) %>%
  #Recode the 88 values to be numeric value of 0. 
  mutate(PHYSHLTH = recode(PHYSHLTH, '88' = 0L))


```

-   The histogram showed most people have between 0 and 10 unhealthy days per 30 days.

-   Next, evaluate mean, median, and mode for the PHYSHLTH variable after ignoring the blanks.

```{r}
mean(brfss.cleaned$PHYSHLTH, na.rm=TRUE)
median(brfss.cleaned$PHYSHLTH, na.rm=TRUE)
names(x = sort(x = table(brfss.cleaned$PHYSHLTH), decreasing = TRUE))[1]
```

-   While the mean is higher at 4.22, the median and most common number is 0.

```{r}
## Spread to Report with the Mean
var(brfss.cleaned$PHYSHLTH, na.rm=TRUE)
sd(brfss.cleaned$PHYSHLTH, na.rm=TRUE)
##Spread to Report with Median
summary(brfss.cleaned$PHYSHLTH, na.rm=TRUE)
range(brfss.cleaned$PHYSHLTH, na.rm=TRUE)
max(brfss.cleaned$PHYSHLTH, na.rm=TRUE)-min(brfss.cleaned$PHYSHLTH, na.rm=TRUE)
IQR(brfss.cleaned$PHYSHLTH, na.rm=TRUE)
```

```{r}
library(semTools)
# Plot the data
brfss.cleaned %>% 
  ggplot(aes(PHYSHLTH)) + geom_histogram()
# Calculate Skewness and Kurtosis
skew(brfss.cleaned$PHYSHLTH)
kurtosis(brfss.cleaned$PHYSHLTH)

```

-   The skew results provide a z of 607.905 (6.079054e+02) which is much higher than 7 (for large datasets). This indicates a clear right skew which means the data is not normally distributed.
-   The kurtosis results are also very leptokurtic with a score of 478.063.

#### Using Filters Example

-   Below takes an example of the brfss data to filter by certain variable statuses.

    -   The first filter() chose observations that were any one of the three categories of transgender included in the data. Used the \| “or” operator for this filter().
    -   The second filter chose people in an age category above category 4 but below category 12, in the age categories 5 through 11.
    -   The last filter used the !is.na to choose observations where HADMAM variable was not NA.

-   Next, we reduce data set to contain only variables used to create table by using the select() command.

-   Next, we change all the remaining variables in data set to factors using mutate_all() command. This not only changes the strings to factors, but also changes the numerical variables to factors.

-   Finally, we use mutate() commands to change the variable category to something meaningful(from the codebook).

    -   Notice the backslash before the apostrophe in Don't in the X_INCOMG recode. This is to prevent the .R file from ending the quotations. You could use double quotes around the statement to bypass this, or add the backslash like I did here.

    ```{r}
    brfss_small <- brfss.cleaned %>%
      filter(TRNSGNDR == 'Male to female'|
            TRNSGNDR ==  'Female to male'|
            TRNSGNDR ==  'Gender non-conforming') %>%
      filter(X_AGEG5YR > 4 & X_AGEG5YR < 12) %>% 
      filter(!is.na(HADMAM)) %>%
      select(TRNSGNDR, X_AGEG5YR, X_RACE, X_INCOMG, X_EDUCAG, HLTHPLN1, HADMAM) %>%
      mutate_all(as.factor) %>%
      #The next few mutates add labels to categorical variables based on the codebook. 
      mutate(X_AGEG5YR = recode_factor(X_AGEG5YR,
              '5' = '40-44',
              '6' = '45-49',
              '7' = '50-54',
              '8' = '55-59',
              '9' = '60-64',
              '10' = '65-69',
              '11' = '70-74')) %>%
      mutate(X_INCOMG = recode_factor(X_INCOMG,
              '1' = 'Less than 15,000',
              '2' = '15,000 to less than 25,000',
              '3' = '25,000 to less than 35,000',
              '4' = '35,000 to less than 50,000',
              '5' = '50,000 or more',
              '9' = 'Don\'t know/not sure/missing')) %>%
         mutate(X_EDUCAG = recode_factor(X_EDUCAG,
              '1' = 'Did not graduate high school',
              '2' = 'Graduated high school',
              '3' = 'Attended college/technical school',
              '4' = 'Graduated from college/technical school',
              '9' = NA_character_)) %>%
         mutate(HLTHPLN1 = recode_factor(HLTHPLN1,
              '1' = 'Yes',
              '2' = 'No',
              '7' = 'Don\'t know/not sure/missing',
              '9' = 'Refused')) %>%
         mutate(X_RACE = recode_factor(X_RACE,
              '1' = 'White',
              '2' = 'Black',
              '3' = 'Native American',
              '4' = 'Asian/Pacific Islander',
              '5' = 'Other',
              '6' = 'Other',
              '7' = 'Other',
              '8' = 'Other',
              '9' = 'Other'))
    #print a summary
    summary(brfss_small)     
    ```

-   This data set full of categorical variables is now fully cleaned and ready to be analyzed!

## Using AI

-   Use the following prompts on a generative AI, like chatGPT, to learn more about data preparation activities.

-   What is the difference between ordinal and nominal variables, and how can you recode these data types in R?

-   How can you use the filter() function to subset a dataset based on multiple conditions using & and \| in R?

-   How does subsetting using square brackets \[\] differ from using the filter() function in R?

-   What strategies can you use to handle missing data in R, and how does na.omit() differ from drop_na()?

-   How does the mutate() function help in transforming and creating new variables, and what are some practical examples?"

-   What is the purpose of the group_by() function, and how does it interact with summarize() to create summary statistics in R?"

-   Explain how you can use arrange() to sort a dataset by one or more variables and demonstrate sorting both in ascending and descending order."

-   Why is the piping operator %\>% useful in R, and how does it improve the readability and structure of your code?

-   How would you use summarize() to calculate mean, median, and standard deviation for a numerical variable in R?

## Summary

-   In this lesson, we worked through the basics on data cleaning. Data cleaning is so important and there are so many ways to do it. Provided are some examples using popular functions in dplyr (under tidyverse).


---
title: "Data Visualization"
format: html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, tidy.opts = list(width.cutoff = 70), tidy = TRUE, message=FALSE, warning=FALSE)
```

-   The goals this lesson are to be able to create visualizations which help to describe a variable or variables more easily (descriptive statistics) and to explore patterns in the data based on groups or between two or more variables (inferential statistics).



### At a Glance

-   In order to succeed in this lesson, we need to learn how to visualize data, noting that there are many ways to do this in R. This lesson presents common graphs used in R, and focuses on using ggplot2, a very common graphics package that is part of the tidyverse.

-   Because packages evolve fairly quickly, parameters within the commands shown in this section could evolve and adjust. The best way to figure out how to fine-tune your new graph is to explore options and parameter settings, while also using the open-source community.

-   By the end of this lesson, you should be able to create the following:

1.  Common Graphs for a Single *Categorical* Variable
2.  Common Graphs for a Single *Continuous* Variable
3.  Common Graphs for Two Variables at Once

-   This lesson does not provide an exhaustive list of what you can do in R, but is a good starting point presenting some of the top graphs used in statistics with some common parameters added in.

### Lesson Objectives

-   Choose and create graphs for a single categorical variable.
-   Choose and create graphs for a single continuous variable.
-   Choose and create graphs for two variables at once.

### Consider While Reading

-   It is extremely useful to graph data in R in order to visualize what the data is telling us.

-   When making graphs, there are a lot of ways to visualize data correctly, but there are just as many to do it incorrectly. Be careful not to force the data into the story you want it to tell.

-   Always be cautious when looking at visuals to make sure the data was handled properly.

-   The lesson is arranged by first evaluating the variable or variables of interest by data type (quantitative or quantitative) and then giving some examples of visualizations where this is commonly used. It is essential to make sure the variable is in the right data type before you create a chart. We look at many charts in this lesson including bar charts, density plots, boxplots and scatterplots. A multiple boxplot helps us understand the relationship between a grouping variable and a numerical variable. The scatterplot helps us understand if there is a relationship between two numerical variables and it helps us determine whether that relationship is linear or nonlinear.

-   A lot of graphs you can do in ggplot, you can also do with base R. However, ggplot has much more capability to customize a specific format for your graphic.

    -   Base R has many options to graph a variable or variables. If all you want is some insight into the data, this might be the way to go.
    -   ggplot2 is extremely popular graphing package and has a good pattern for making charts. You set the first layer with the ggplot() command and then add layers as needed for your graphic.
    -   There are also other graphing packages that serve a specific purpose (i.e., waffle package in R).

## Layering in ggplot

-   Layering is a fundamental concept in ggplot2 that allows you to build complex visualizations by adding different components (or "layers") on top of each other. Each layer in a ggplot2 plot can represent different types of data, aesthetics, or annotations, enabling flexibility and control over how data is visualized. Layers can include geometries (e.g., points, lines, bars), statistical summaries, labels, grids, and themes.
-   Separation of Plot Components: Each layer can handle a different part of the plot. For example, one layer may be used for bars, another for labels, and another for a trend line. This allows you to build up a plot step by step.
-   Customization and Enhancement: By adding multiple layers, you can customize different aspects of the plot such as labels, colors, annotations, and theme elements. Each layer can be independently controlled.
-   Modularity: Layering allows you to modularize your plot construction, making it easier to add, remove, or modify parts of the plot without changing the entire structure.
-   Combining Data Sources: Different layers can use different datasets or aesthetics, which is useful when you need to overlay one dataset on top of another (e.g., adding a regression line over a scatter plot).

## Graphs for a Single Categorical Variable

-   A categorical variable has categories that are either ordinal with a logical order or nominal with no logical order.
-   Categorical variables need to be set as the factor data type in R to be able to be analyzed and visualized correctly.
-   Some common graphing options for single categorical variable:
    1.  Bar graph
    2.  Pie chart
-   In any graph, it is beneficial to do any data cleaning and investigation into the variable(s) before you begin. With categorical variables, this may require recoding the factor(s) of interest and possibly renaming it/them to something meaningful if needed.

### Bar Graph

-   A bar graph depicts the frequency or the relative frequency for each category of the qualitative data as a bar rising vertically from the horizontal axis. A bar graph is also known as a bar chart and is often used to examine similarities and differences across categories of things; bars can represent frequencies, percentages, means, or other statistics.
-   We can learn a lot from a bar graph, like the marital status group with the highest and lowest frequencies according to the census.gov.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.alt = "Bar Chart Example"}
library("tidyverse")
```

```{r, echo=FALSE}
Categories<- c("Never married", "Married", "Separated", "Widowed", "Divorced")
Value <- c(89028651, 127892670, 4912426, 15230473, 28767947)
barp <- data.frame(Categories, Value)
options(scipen=999)
ggplot(barp, aes(x=Categories, y=Value, fill=Categories))+geom_bar(stat="identity")+
  labs(x = "Marital Status", y = "Estimate of Marital Status (15 years and older)") +
  ggtitle("2020: ACS 5-Year Estimates Data Profiles from data.census.gov/")
```

#### geom_bar()

-   Create a Bar Graph using ggplot() Command
-   Like histograms, ggplot has many more parameters available over base R to construct bar graphs.
-   First, we load tidyerse to access ggplot() command and others. You can always do this at the start of all your code to keep all the libraries together that are being used.

```{r, message=FALSE}
library("tidyverse")
```

-   ggplot() works in layers, so you will routinely see the *+* symbol to kick off a new layer with added functionality.
-   Using the ggplot, we always include the aes() command first inside the ggplot() command. The aes() command is a *quoting function* that describes the variables being used. From there, it depends on the plot.
    -   First layer: ggplot() and aes() which calls the dataset and variables used.
    -   Second layer: Graph type: Bar graph: geom_bar().
    -   Additional layers: labs - for labels including titles; themes; and geom_text. Recreate the example below adding one layer at a time to see how the visualization changes.

#### Pertinent Parameters to the geom_bar()

-   **stat="identity"**: In ggplot2, the argument *stat="identity"* is used inside the geom_bar when creating bar charts to specify that the actual values of the data should be plotted, rather than calculated summary statistics. By default, ggplot assumes that bar charts use stat="count", which counts the number of observations in each category. When you want to display the actual values in your dataset (e.g., sales numbers, average scores), you need to set stat="identity" to ensure that the y-values are mapped directly from the data rather than being automatically counted.
-   **position="dodge"**: If we have more than one categorical variable, we might set the *position="dodge"* argument. This argument controls how the bars are positioned relative to each other when you are plotting multiple categories within a bar chart. By default, bars in a bar chart are stacked, and setting position="dodge" ensures that they are placed side-by-side. This is particularly useful when you are comparing multiple groups or categories within the same plot, as it allows for a clear visual distinction between each group. In the example below, we only have one variable, so position="dodge" argument is not needed.\
-   In combination, using *stat="identity"* and *position="dodge"* is common when you want to compare different categories or groups with specific values in a side-by-side manner, ensuring the chart is easy to interpret. For example, if you are comparing sales figures across different products for multiple years, this approach would give you a bar for each product in each year, clearly separated.
-   **show.legend = FALSE**: In ggplot2, the argument *show.legend = FALSE* is used to hide the legend for a particular layer or the entire plot. By default, ggplot2 automatically adds a legend if you include aesthetics like color, size, or shape that distinguish groups in your data. If you don't want the legend to appear, you can set show.legend = FALSE.
-   **scale_fill_manual():** When creating grouped bar charts or other plots with multiple categories, it's important to ensure that the visual distinction between groups is clear. This is where manually setting the colors for each category comes into play. The *scale_fill_manual()* function in ggplot2 allows you to manually define the colors used for the filled areas, such as the bars in a bar chart or the shaded areas in an area chart. By using the values argument within scale_fill_manual(), you can specify a custom color palette that suits your design or presentation needs. For example, you might choose a palette of yellow and brown to represent different categories. A key aspect of using scale_fill_manual() effectively is knowing how many colors to provide. The number of colors you define in the values argument must match the number of levels or categories in your data. If you are comparing three product categories, for instance, you'll need to provide exactly three colors—one for each category. Failing to match the number of colors to the number of categories can result in errors or misrepresentation in the plot. For example, if you have four levels (e.g., four different product categories or groups) and only provide two colors, ggplot may not know how to properly assign colors to the additional categories. Therefore, it's crucial to know the number of distinct categories in your data and plan your color palette accordingly to maintain clarity and visual consistency in your chart.

```{r}
##inputting probabilities calculated from a 2023 multiple choice question. 
# From what you learned about R so far, how do you expect its market share to change?
GoUp <- .54285
GoDown <- .03809
RemainStable <- .34285
NoOpinion <- .07619
#designing the data frame
data_frame <- data.frame(
     Category = c("Go Up", "Go Down", "Remain Stable", "No Opinion"),
     Percentage = c(GoUp, GoDown, RemainStable, NoOpinion)
)
#Making the graph
MarketShare <- ggplot(data_frame, aes(x = Category, y = Percentage, fill = Category)) +
     geom_bar(stat = "identity", show.legend = FALSE) +
     labs(title = "How do you expect R's market share to change?",
          x = "Opinion Category",
          y = "Percentage (%)") +
     theme_minimal()+
     geom_text(aes(label = Percentage), vjust = -0.5, size = 4) + 
     scale_fill_manual(values=c("red", "blue", "purple", "green"))

MarketShare
```

### Bar Graph with Data Wrangling

-   Let's start an example from scratch so that we can see each parameter take effect. In doing so, lets use a dataset to make a bar graph instead of relying on pre-calculated data.\
-   Lets examine the AUQ300 variable from the nhanes survey to run an example.

```{r}
nhanes <- read.csv("data/nhanes2012.csv")
```

-   Next, we need to check the import by looking at the summary or head of the data.

```{r, results='hide'}
#Results hidden to save space, but gives you the first 6 records in the data set. 
head(nhanes)
```

-   We can also check the summary of data of only the variable of interest, AUQ300, to get a sense of what we are evaluating.

```{r}
summary(nhanes$AUQ300) 
```

-   The AUQ300 variable represents gun use. A screenshot of the codebook is copied below so that we can see what AUQ300 really refers to. It is available on https://wwwn.cdc.gov/Nchs/Nhanes/2011-2012/AUQ_G.htm. This is always a necessary step because variable names can be convoluted and not representative of the variable definition.

![AUQ300](Pictures/Ch3/AUQ300.png "Snippit of the AUQ300 Variable in the Codebook")

#### Recode Variable if Needed

-   Look to see if the AUQ300 needs recoding after looking at the codebook and making sense of the variable.
-   AUQ300 needs to be a factor variable with 1 equaling a *Yes* and 2 equaling a *No*. We can use recode_factor to accomplish 2 things at once with the mutate function.
-   recode_factor() transforms the levels of a categorical variable (factor) into a new set of levels and is specific to categorical variables.
-   recode() is generic and can apply to numerical, categorical, or textual data, but still transforms data from one format or code to another.

```{r}
nhanes.clean <- nhanes %>%
  select(AUQ300) %>%
  mutate(AUQ300 = recode_factor(AUQ300,
    '1' = 'Yes',
    '2' = 'No'))
```

-   Then, we need to check the recode for accuracy. You should see the No's and Yes's alongside the rest being coded as NA's.

```{r}
summary(nhanes.clean)
```

#### Get Bar Roughly Plotted

-   Start with the basic plot using the ggplot() and geom_bar() commands.

-   Below writes the statement with and without the piping operator.

    -   Since we are also going to use data preparation techniques, the piping operator is recommended.

    ```{r, fig.alt = "Bar Graph of AUQ300 Variable (Gun Use) Generated by R"}
    # Without piping operator
    ggplot(nhanes.clean, aes(x = AUQ300)) + geom_bar()
    #With piping operator
    nhanes.clean %>%
      ggplot(aes(x = AUQ300)) + geom_bar()

    ```

#### Add Functions to Clean Chart

-   Omit the NA category from AUQ300 variable, which represents gun use. Then plot the graph below.
-   The drop_na() function is a good way to drop NA values from either the entire dataset or just one variable. It was introduced in the data prep lesson. Since we are only interested in dropping NA values from our one variable of interest that is to be graphed (AUQ300), we can put it in the parentheses so that we do not unintentionally drop lots of observations for no reason.
-   Add an axis labels under *labs(x = ..., y=...)*.

```{r tidy=FALSE, fig.alt = "Bar Graph of AUQ300 Variable (Gun Use) with labels"}
nhanes.clean %>%
  drop_na(AUQ300) %>%
  ggplot(aes(x = AUQ300)) + geom_bar() +
  labs(x = "Gun use", y = "Number of participants")
##Here, we really benefit from the piping operator because we are doing more than one thing.
```

-   From the bar graph, we can see that almost double the amount of people have not fired a firearm for any reason than those that fired one.

#### Adding Color

-   There are many ways to add color to a bar graph. Below, the color is filled in directly in the aes() command by choosing it to give a different color to each categorical value of AUQ300.

    -   When fill is mapped to a variable, the fill color of the geom will vary based on the values of that variable. This is useful for distinguishing different groups or categories within the data. In this case the fill=AUQ300 gives a distinct color pattern based on how many categories there are considering the fact we are using the default "scale."

    ```{r tidy=FALSE, fig.alt = "Bar Graph of AUQ300 Variable (Gun Use) with color"}
    nhanes.clean %>%
      drop_na(AUQ300) %>%
      ggplot(aes(AUQ300, fill=AUQ300)) +
      geom_bar() +
      labs(x = "Gun use", y = "Number of participants", 
           subtitle = "Filled inside the aes()") 
    ```

#### Data Prep and Then Visualized

-   In the command below, we create a gss.2016.cleaned object to make a barplot. In doing so, we do the following:

-   Create a bar graph using the ggplot() command, which requires an aes() quoting function. This function says that we want to use the grass variable in our bar graph.

-   Drop all NAs from the grass variable so that legal and not legal are the only categories.

-   We then create the bars and fill them with 2 colors, red and purple. Many color codes can be used here, and will be discussed in a later lesson.

-   We then add labels to our graph on both x and y axis.

-   Finally, we print the new graph, which is saved under the legalize.bar object.

-   Below, I brought back over the code from the last part in Data Preparation. You should still have this in your Chapter1.R file. We are going to use that file to create a graphics in R.

```{r, message=FALSE}
gss.2016 <- read_csv(file = "data/gss2016.csv") 
gss.2016.cleaned <- gss.2016 %>% 
  mutate(grass = as.factor(grass)) %>% 
  mutate(grass = na_if(x = grass, y = "DK")) %>% 
  mutate(grass = na_if(x = grass, y = "IAP")) %>% 
  mutate(grass = droplevels(x = grass)) %>% 
  mutate(age = recode(age, "89 OR OLDER" = "89")) %>% 
  mutate(age = as.numeric(x = age)) %>% 
  mutate(age.cat = cut(x = age, breaks = c(-Inf, 29, 59, 74, Inf),labels = c("< 30", "30 - 59", "60 - 74", "75+" ))) 
```

-   Once the data is prepped, we can graph the variable or variables.

```{r}
ggplot(gss.2016.cleaned, aes(grass)) + geom_bar() ##with no piping operator
gss.2016.cleaned %>% ggplot(aes(grass)) + geom_bar() ##with piping operator
```

```{r, fig.alt = "Bar Graph Generated by R"}
# Make a Bar Graph for Grass Variable
gss.2016.cleaned %>% 
     drop_na() %>%
     ggplot(aes(grass)) + geom_bar(fill=c("red", "blue")) + 
     labs(x = "Should marijuana be legal", y="Frequency of Responses")

```

#### Edit The Graphic

-   Next, we can edit these commands to include the age variable. The aes() quoting function has expanded to have the bars filled color using the grass variable, the age category has replaced the grass variable on the x axis.

```{r}
gss.2016.cleaned %>% 
     drop_na() %>%
     ggplot(aes(age.cat, fill=grass)) + geom_bar() + labs(x="Age Category", y="Frequency of responses")
```

-   We can add the position set at "dodge" inside the geom_bar() layer to make the barchart unstacked (or grouped).

```{r}
gss.2016.cleaned %>% 
     drop_na() %>%
     ggplot(aes(age.cat, fill=grass)) + geom_bar(position="dodge") + labs(x="Age Category", y="Frequency of responses")
```

-   We can edit further to include a new a formula on the y axis to sum and count.
-   In the formula you provided, after_stat(count) is used within the ggplot2 framework to refer to the computed statistic generated by the geom_bar() function. Specifically, in the context of bar plots, count refers to the number of observations (or frequency) for each category within the data.
-   When you use after_stat(count), you are referencing the count that is computed after geom_bar() has processed the data and calculated how many observations fall into each group (in this case, within each age category and the "grass" variable, which likely refers to attitudes toward marijuana legalization).
-   We also gave this a theme and updated the labels.

```{r, fig.alt = "Grouped Bar Graph Generated by R"}
gss.2016.cleaned %>% 
     drop_na() %>%
     ggplot(aes(age.cat, y = 100*(after_stat(count))/sum(after_stat(count)), 
                fill=grass)) + 
     geom_bar(position = 'dodge')+  
     theme_minimal()+ 
     labs(x = "Age Category",y = "Percent of responses")

```

-   Evaluate these graphs and see what information you can get from them.

### Pie Chart

-   A pie chart is a segmented circle whose segments portray the relative frequencies of the categories of a qualitative variable.
-   Slices of pie in different colors represent the parts.
-   In this example, the firearm is divided by type to show parts of a whole, where the total of the proportions must add to 1.0 and the total of the percentages must add to 100%.

```{r}
# Importing data from working directory
fbi.deaths <- read.csv("data/fbi_deaths.csv", stringsAsFactors = TRUE)
# Selecting rows of interest for pie chart
fbi.deaths.small <- fbi.deaths[c(3,4,5,6,7),]
fbi.deaths.small <- fbi.deaths.small %>%
  rename(Weapon = X)
# Checking summary of fbi deaths
summary(fbi.deaths.small)
```

-   Again, ggplot works in layers, so in order to make a pie, you need a few layers and have a few optional ones.
    -   The aes() command specifies the variable to create the pie, in this case x2016.
    -   geom_col() sets the borders of the pie and makes it visible.
    -   coord_polar() command makes the pie circular.
    -   theme_void() command is optional and adjusts the theme of the pie. to remove axis, background, etc.

```{r tidy=FALSE, fig.alt = "Pie Chart Generated by R"}
ggplot(fbi.deaths.small, aes(x="", y=X2016, fill=Weapon)) +
  geom_col() + 
  coord_polar("y", start=0) + 
  theme_void() 
```

-   From the pie, we can see that the majority of weapons that caused fbi gun related deaths are handguns followed by a type of firearm that is not stated.

### Comparison of Charts

-   Recommended graphs for single categorical or factor type variable:
    -   Bar graph, for showing relative group sizes.
    -   Pie charts are available in R but are not recommended because they tend to be less clear for comparing group sizes.
        -   Pie charts are difficult to read since the relative size of pie pieces is often hard to determine.
        -   Pie charts take up a lot of space to convey little information.
        -   People often use fancy formatting like 3D, which takes up more space and makes understanding relative size of pie pieces even more difficult.

## Graphs for a Single Continuous Variable

-   A continuous variable refers to a variable that can take any value over a range of values.
-   A continuous variable needs to be numeric, and could be integer type or numeric type in R. Just like with graphs that include categorical variables, it is beneficial to do any data cleaning and investigation into the variable(s) before you begin. With continuous variables, this may require recoding the variable to coerce it to the appropriate data type and/or renaming it to something meaningful if needed.
-   It is also beneficial to make sure the numerical variable is indeed supposed to be numerical (as opposed to a factor). For instance, you commonly see numbers listed for categories like the Yes/No coded as a 1/2, such as with the AUQ300 variable.

Some common graphing options for single continuous variable:

-   Histograms (From Lesson 2)
-   Density plots
-   Boxplots
-   Violin plots

### Histograms

-   A histogram is a useful plot to determine central tendency and spread.

-   We went over histograms in Lesson 2, so refer back for information on how to create a histogram using base R and ggplot.

-   Remember that you can tell the distribution from a histogram, and that distribution can be normal or skewed (Right or Left).\

-   With each chart based on quantitative data, you should be able to get a sense of the distribution.

-   The histogram below looks right skewed.

```{r, message=FALSE, echo=FALSE}
x <- rnorm(500, 10, 2)
x <- x^3
library(tidyverse)
df <- as.data.frame(x)
ggplot(df, aes(x)) + geom_histogram(color="purple", fill="lavender", binwidth = 100)+ggtitle("A Right Skewed Histogram")
```

### Density Plots

-   A density plot is similar to a histogram but more fluid in appearance because it does not have the separate bins.\

-   *Probability density* is not very useful for interpreting what is happening at any given value of the variable on the x-axis, but it is useful in computing the percentage of values that are within a range along the x-axis.

-   The *area under the curve* in a density plot could be interpreted as the probability of a single observation or a range of observations.

-   We can use random normal data to create the density plot like shown below with a sample of 1000, a mean of 10 and a standard deviation of 2. To do this, we need to make the vector and assign it to a data frame.

-   In R, set.seed() is a function used to set the seed for random number generation. By setting a seed using set.seed(), you ensure reproducibility of your code. If you run the same code with the same seed, you'll get the same sequence of random numbers every time. This is particularly useful for debugging, testing, or when you want to ensure that your results are reproducible.

-   We use set.seed before any function with a random normal generator to ensure reproducibility.

-   If a dataset is provided, then you do not need to generate your own random data as shown in the step below.\

```{r}
set.seed(1)
x <- rnorm(1000, mean = 10, sd=2)
df <- data.frame(x)
```

-   Next, we can make the density plot using the ggplot2 package under tidyverse.
    -   Layer 2 includes the geom_density() command in addition to the standard Layer 1 ggplot() command to create the density plot.

```{r, fig.alt = "Density Chart Generated by R"}

ggplot(df, aes(x)) + geom_density()
 
```

-   There are a lot of arguments you can change. I selected a couple below. Be sure to look at the help file on the geom_density() layer to get the variety on what you can do.

    -   color = sets a line color
    -   lwd = makes the line thicker. Increase this number for thicker line.
    -   fill= colors the area under the curve.
    -   alpha= sets the transparency to the area under the curve.

```{r}
ggplot(df, aes(x)) + geom_density(color = "darkblue", lwd = 2)

```

```{r, fig.alt = "Density Chart with Styles Generated by R"}

ggplot(df, aes(x)) + geom_density(color = "darkblue", fill = "lightblue",alpha = .5) 

```

-   We can even add a mean line, which we know in this case is 10 because we used random normal data with that mean set as a parameter.
-   geom_vline() is a function used to add vertical lines to a plot created with ggplot. This function is useful for visually indicating specific points or ranges on the x-axis.
-   You can do a line break in your R code after a comma ($,$) or after a plus sign ($+$). I find things easier to read on less lines, but it is personal preference how many lines you use given still following the rules in R.\

```{r, fig.alt = "Density Chart with Mean Line Generated by R"}
ggplot(df, aes(x)) +
  geom_density(color = "darkblue", fill="lightblue", alpha=.5) + 
  geom_vline(aes(xintercept=mean(x)),
     color="red", linetype="dashed", lwd=1)
```

-   You do not need the rnorm function if you are provided a dataset with a numerical variable. The following code uses the customers dataset to do 2 examples of density plots with 2 numerical variables.

```{r}
customers <- read.csv("data/customers.csv")

str(customers)

ggplot(customers, aes(Income)) + geom_density()
ggplot(customers, aes(Orders)) + geom_density(color = "#745033",fill="#740000",alpha=.5)
```

### Boxplot

-   A boxplot is a visual representation of data that shows central tendency (usually the median) and spread (usually the interquartile range) of a numeric variable for one or more groups.

-   Boxplots are often used to compare the distribution of a continuous variable across several groups.

-   A box plot allows you to:

    -   Graphically display the distribution of a data set.
    -   Compare two or more distributions.
    -   Identify outliers in a data set.

![A Boxplot with Outliers on Left](Pictures/Ch3/Boxplot.png "A Boxplot with Outliers on Left")

-   Boxplots include the following information:
    -   A line representing the median value.
    -   A box containing the middle 50% of values.
    -   Whiskers extending to 1.5 times the IQR.
    -   Outliers more than 1.5 times the IQR away from the median.

![A Boxplot with No Outliers](Pictures/Ch3/Boxplot2.png "A Boxplot with No Outliers")

-   This boxplot above displays 5 summary values:
    -   S = smallest value.
    -   L = largest value.
    -   Q1 = first quantile = 25th percentile.
    -   Q2 = median = second quantile = 50th percentile.
    -   Q3 = third quantile = 75th percentile.
-   For example, use the GrowthFund Vector from the last lesson. It is executed again below.

```{r}
GrowthFund <- c(-38.32, 1.71, 3.17, 5.99, 12.56, 13.47, 16.89, 16.96, 32.16, 36.29)
GrowthFund <- as.data.frame(GrowthFund)
```

-   We can use ggplot to retrieve our graph and associated numbers.
-   The outlier is visually depicted on the graph as -38.32.

```{r, fig.alt = "Boxplot Generated by R"}
ggplot(GrowthFund, aes(GrowthFund)) + geom_boxplot()
```

-   We can add a little color to the plot with the fill parameter, but then we also need to be sure to turn off the legends in the geom_boxplot.

```{r, fig.alt = "Boxplot Generated by R"}
ggplot(GrowthFund, aes(GrowthFund)) + geom_boxplot(fill="red")
```

-   You can add parameters to make this visualization more professional, but this gets you started. Be sure to look at some examples in the R community or on ChatGPT.


* Another example below using the mtcars dataset. 
```{r tidy=FALSE, fig.alt = "Box Plot with Boxplot Center Generated from ggplot2 by R"}
mtcars %>%
  ggplot(aes(x="", y = mpg)) +
  geom_boxplot(fill="lightgreen") +
  theme_minimal() 

```


```{r}
semTools::skew(mtcars$mpg) #normal
semTools::kurtosis(mtcars$mpg) #mesokurtic
```

-   Looks like the mpg variable is quite normal with one potential outlier to the right, but no major signs of skewness or kurtosis.

-   We can also get a histogram of mpg and are able to make the same claims towards normality. You can see a slight pull to the right, but it is seemingly normal. A higher sample size could help here.

```{r}
ggplot(mtcars, aes(mpg)) + geom_histogram(binwidth = 5, color="black", fill="green")
```

## Graphs for Two Variables At Once

-   Combinations of 2 Variable Types for Graphing
    -   Two categorical/ factor.
    -   One categorical/ factor and one continuous/ numeric.
    -   Two continuous/ numeric.

### Bar Graphs for Two Categorical Variables

-   There are two formats available for bar charts:
    -   Grouped
    -   Stacked

```{r, echo=FALSE}
countsDF <- mtcars %>% 
     group_by(vs, gear) %>%
     count()
countsDF

ggplot(countsDF, aes(x = gear, y = n, fill = factor(vs))) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Grouped") + 
  scale_fill_manual(values = c("darkblue", "red"),
                    labels = c("vs-0", "vs-1")) 

ggplot(countsDF, aes(x = gear, y = n, fill = factor(vs))) +
  geom_bar(stat = "identity") +
  labs(title = "Stacked") + 
  scale_fill_manual(values = c("darkblue", "red"),
                    labels = c("vs-0", "vs-1")) 

```

#### Grouped Bar Graph

-   Grouped bar graph allow comparison of multiple sets of data items, with a single color used to denote a specific series across all sets.

-   For example, we can look at both the vs and gear variables in the ggplot command.

    -   You can do a little grouping and counting before you began to generate a new table with frequencies based on vs and gear variables. Once a new dataset object is made, you can make the graph with the geom_bar layer specifying the stat="identity".
    -   Since there are two variables, you can set the position to *dodge* to view the fill categorical variable side by side.
    -   (stat = "identity") tells ggplot that the y values are already computed and should be used as-is for the heights of the bars. In this case, they are frequencies calculated in the countsDF dataset.

```{r fig.alt = "Grouped Bar Graph Generated by R"}
    
mtcars <- mtcars %>% 
    mutate(vs=as.factor(vs)) %>% 
    mutate(gear=as.factor(gear))


countsDF <- mtcars %>% 
    group_by(vs, gear) %>%
    count()

summary(countsDF)
    
ggplot(countsDF, aes(x = gear, y = n, fill = vs)) +
     geom_bar(stat = "identity", position = "dodge") +
     labs(title = "Grouped Car Distribution by Gears and VS",
     x = "Number of Gears", y = "Count") +
     theme_minimal()
```

#### Stacked Bar Graph

-   A Stacked bar graph extends the standard bar graph from looking at numeric values across one categorical variable to two. Each bar in a standard bar graph is divided into a number of sub-bars stacked end to end, each one corresponding to a level of the second categorical variable.
-   Using ggplot, we can also stack these charts by removing the position = dodge statement.

```{r tidy=FALSE, fig.alt = "Stacked Bar Graph Generated from ggplot2 by R"}
ggplot(countsDF, aes(x = gear, y = n, fill = vs)) +
  geom_bar(stat = "identity") +
  labs(title = "Stacked Car Distribution",
       x = "Number of Gears",
       y = "Count") +
  theme_minimal()
```

#### Bar Graph for Continuous Across Groups

-   In comparison to a bar graph for a single categorical variable, a bar chart for a continuous variable across groups includes both a x and y axis. The continuous variable is put on the y axis, and the categorical (factor) variable is placed on the x axis showing the groups.

-   Therefore, instead of counting data based on group, we can see another continuous variable based on group data.

-   The frequency data (i.e., counts) can be replaced with another numerical variable like mean.

-   In the below example, instead of counting observations per group, here, we took the average mpg (a continuous variable) based on groups of gear and vs and summarized the data into a variable avg_mpg. We then used that variable in a ggplot() command to create a unique chart to that above.

```{r message=FALSE}
avg_mpg <- mtcars %>%
  group_by(gear, vs) %>%
  summarise(mpg = mean(mpg, na.rm = TRUE))
```

```{r tidy=FALSE, fig.alt = "Grouped Bar Graph Generated from ggplot2 by R"}
ggplot(avg_mpg, aes(gear,
  mpg, fill = vs)) +
  geom_bar(stat = "identity", position = "dodge") +
  ggtitle("Average MPG by VS and Gear")
```

-   Below, we can add color using the scale_fill_manual. Since there are two colors, we use the c() command to combine them together inside the layer.

```{r}
ggplot(avg_mpg, aes(gear,
     mpg, fill = vs)) +
     geom_bar(stat = "identity", position = "dodge", color="black") +  
     ggtitle("Average MPG by VS and Gear")+
     scale_fill_manual(values=c("yellow", "brown"))
```

### Boxplot for Continuous Across Groups

-   A boxplot requires one continuous variable (like we did above). When we include an additional grouping variable, we get multiple boxplots, one for each group. This allows us to directly compare distributions.

-   The categorical variable should correctly be a factor data type before you begin.

-   In the example below, mpg is the continuous variable, and gear is the categorical variable.

-   We see 3 boxplots for three values of gear (3, 4, 5). ggplot() and geom_boxplot() are required components of the command. The scale_fill_manual() and theme_minimal() layers are optional ways to change the style and color.

```{r tidy=FALSE, fig.alt = "Boxplot Generated from ggplot2 by R"}
mtcars %>%
  ggplot(aes(x = gear, y = mpg, fill = gear)) +
  geom_boxplot(show.legend = FALSE) +
  scale_fill_manual(values = c("gray", "red", "blue")) +
  theme_minimal()
```

-   Lets alter the functions above to depict mpg based on vs with categorical states 0 and 1.

```{r}
mtcars %>%
     ggplot(aes(x = vs, y = mpg, fill = vs)) +
     geom_boxplot(show.legend = FALSE) +
     scale_fill_manual(values = c("gray", "red")) +
     theme_minimal() 
```

### Scatterplot for Two Continuous Variables

#### Scatterplots

-   A scatterplot is used to determine if two continuous variables are related.
    -   Each point is a pairing: $(x_1, y_1),(x_2, y_2),$ etc.
-   Our goal with a scatterplot is to characterize the relationship by visual inspection. This includes determining if the relationship looks positive, negative, or not existent.

![ScatterPlot Results](Pictures/Ch3/Scatter.png "ScatterPlot Results")

```{r, echo=FALSE, results='hide' }
x <- sort(sample(1:50, replace=TRUE), decreasing=FALSE) 
y <- sort(sample(1:50, replace=TRUE), decreasing=FALSE)
z <- sort(1:50, decreasing=TRUE)
s <- sample(0:50,50, replace=TRUE)
t <- sample(0:50,50, replace=TRUE)

y <- y + rnorm(50, mean=0, sd=5)  # Adding random noise to y
z <- z + rnorm(50, mean=0, sd=5)  # Adding random noise to z
dataF <- data.frame(x,y,z, s, t)
pos <- dataF %>% ggplot(aes(x, y))+ 
     geom_point(color="#183028", shape=16) + 
     geom_smooth(method="lm", color="#789F90") + 
     theme_minimal()+
     labs(x="Positive Linear Relationship")
neg <- dataF %>% ggplot(aes(x,z))+ 
     geom_point(color="#183028", shape=16) + 
     geom_smooth(method="lm", color="#789F90") + 
     theme_minimal()+
     labs(x="Negative Linear Relationship")

no <- dataF %>% ggplot(aes(s, t))+ 
     geom_point(color="#183028", shape=16) + 
     geom_smooth(method="lm", color="#789F90") + 
     theme_minimal()+
     labs(x="No Linear Relationship")

```

-   Sometimes, it is really clear how to characterize the relationship. Other times, additional statistical tests are needed to confirm the relationship (which we will go over in later lessons). This is true especially with big data, where the plot window can look like a giant blog of observations.

-   Let's work a clean example examining the relationship between income and the years of education one has had.

-   This plot has a clear positive trend, meaning that as one has more years of education, we see higher income. And similarly, as we see higher income, we also see more years of education. This means that a scatter can help characterize the relationship, and does *not* state that one variable is causing another to occur.

```{r, fig.alt = "Scatterplot Generated by R"}
Edu <- read.csv("data/education.csv")
plot(Edu$Income ~ Edu$Education, ylab = "Income", xlab = "Education")
```

-   Working with ggplot instead of base R, we would use the following code.

    -   Layer 1: ggplot() command with aes() command directly inside of it pointing to x and y variables.
    -   Layer 2: geom_point() command to add the observations as indicators in the chart.
    -   Layer 3 or more: many other optional additions like labs (for labels) as shown below.

    ```{r, fig.alt = "Scatterplot Generated from ggplot2 by R"}
    ggplot(Edu, aes(x=Education, y=Income)) +
         geom_point() +
         labs(y= "Income", x = "Education") 
    ```

-   geom_point() has some parameters you can change like shape = where you can add depth to your chart. Some common shapes of geom_point are as follows.

```{r}
# shape = 0, square
# shape = 1, circle
# shape = 2, triangle point up
# shape = 3, plus
# shape = 4, cross
# shape = 5, diamond
# shape = 6, triangle point down
# shape = 7, square cross
# shape = 8, star
# shape = 9, diamond plus
# shape = 10, circle plus
# shape = 11, triangles up and down
# shape = 12, square plus
# shape = 13, circle cross
# shape = 14, square and triangle down
# shape = 15, filled square
# shape = 16, filled circle
# shape = 17, filled triangle point-up
# shape = 18, filled diamond
```

-   For instance, the code below changes the color, shape, and size of the geom_point().

```{r, fig.alt = "Scatterplot with Styles Generated from ggplot2 by R"}

ggplot(Edu, aes(x=Education, y=Income)) +
     geom_point(color="#183028", shape=18, size=10) +
     labs(y= "Income", x = "Education") 
```

-   geom_line() in R's ggplot2 package is used to create line plots, which connect data points with straight lines. It is commonly used to visualize trends over time or continuous relationships between variables.
-   ggplot allows us to add a geom_line, which is helpful in drawing a line through the data. Here, I am also resetting the color off of the default value. You see this a lot on time series models like stock charts.

```{r, fig.alt = "Scatterplot With geom_line Generated from ggplot2 by R"}
ggplot(Edu, aes(x=Education, y=Income)) +
    geom_point(color="#183028", shape=18, size=4) +
    labs(y= "Income", x = "Education") +
    geom_line(color="#789F90")
```

-   In R's ggplot2 package, geom_smooth() is used to add a trendline (also called a smoothing line or a regression line) to a plot. It fits a line or curve through your data points based on a smoothing method, allowing you to visualize the general relationship between the x and y variables. The trendline can help reveal patterns such as linear relationships or trends in noisy data. The trendline give us 2 parts:
    -   Trendline: This line represents the fitted relationship between the two variables, which could be linear, polynomial, or a more flexible curve depending on the smoothing method used.
    -   Confidence Interval: The shaded area (often gray by default) around the trendline shows the confidence interval, giving a sense of the uncertainty of the fit.
-   We can change our line to a geom_smooth line, which is considered a trendline to help us visualize the relationship between the variables.

```{r, fig.alt = "Scatterplot With geom_smooth Generated from ggplot2 by R"}
ggplot(Edu, aes(x=Education, y=Income)) +
    geom_point(color="#183028", shape=18, size=4) +
    labs(y= "Income", x = "Education") +
    geom_smooth(color="#789F90")
```

-   We can change the type of trendline. The most common is the to develop the trendline using the lm method, which stands for the linear method that we are going to learn in the Regression lesson. For now, lets insert \_method="lm" into our geom_smooth() later to see the change.
-   The gray band on a scatter plot in R usually represents the confidence interval around a fitted line when you plot it using geom_smooth() in ggplot2. This gray area visualizes the uncertainty or variability of the estimated regression line, providing a sense of how confident we are about the predictions at different points along the line.

```{r, fig.alt = "Scatterplot With geom_smooth lm method Generated from ggplot2 by R"}
ggplot(Edu, aes(x=Education, y=Income)) +
    geom_point() +
    labs(y= "Income", x = "Education") +
    geom_smooth(method="lm", color="#789F90")
```

-   Unlike geom_smooth(), which fits a trendline or smoothing line to data, geom_line() directly connects the raw data points in the order they appear (typically by the x-axis values). In business statistics class that teaches linear regression, we use more trendlines than geom_lines.

-   Let's look at a few more examples and see if the relationship is considered positive, negative, or not existent.

-   Below, we see a negative trend.

```{r, fig.alt = "Scatterplot examples in R"}
ggplot(mtcars, aes(x=disp, y=mpg))+ 
  geom_point() + 
  geom_smooth(method="lm", color="#789F90")
```

-   In regards to the hp variable, below, we see another negative trend.

```{r, fig.alt = "Scatterplot examples in R"}
ggplot(mtcars, aes(x=hp, y=mpg))+ geom_point() +
  geom_smooth(method="lm", color="#789F90")
```

-   In regards to the qsec variable, below, we see a weak positive trend. This relationship would need to verified later on.

```{r, fig.alt = "Scatterplot examples in R"}
ggplot(mtcars, aes(x=qsec, y=mpg))+geom_point() + geom_smooth(method="lm", color="#789F90")
```

-   The plot() command also works when you do not have 2 continuous variables, and instead have one categorical variable paired with one continuous variable. However, the plot is not as adequate as others are in inferring the relationship from the variables.
-   For example, the plot below would be better served as a boxplot.

```{r, fig.alt = c("Plot Example with Incorrect Data Type", "Plot Example in Boxplot to Show Comparison")}
plot(mtcars$mpg~ mtcars$cyl)  
boxplot(mtcars$mpg~ mtcars$cyl)
```

-   Using ggplot, we would have the same issue. Since vs is a categorical variable, it does not look right when we use the geom_point() later.

```{r, fig.alt = "ggplot2 Example with Incorrect Data Type"}
ggplot(mtcars, aes(cyl, mpg)) + geom_point() + geom_smooth(method="lm")
```

-   Instead, we would want the geom_boxplot() layer like shown below. The geom_smooth() is also not an applicable layer to a boxplot and would need to be removed.\
-   When you create multiple boxplots in ggplot2 and one or more variables are numeric but not converted to factors, only one boxplot may show up because ggplot2 treats numeric variables as continuous. Boxplots group data by categorical variables, so if your variable is continuous, ggplot interprets it as a single category and produces only one boxplot.
-   Below, we still have an error.

```{r, fig.alt = "ggplot2 Example with Boxplot"}
ggplot(mtcars, aes(cyl, mpg)) + geom_boxplot()


```

-   Something still is not quite right here, so we would need to make sure cyl is correctly a factor before making the boxplot. Now, we should see multiple boxplots, one for each categorical level. To fix this, you need to convert the grouping variable to a factor using as.factor() in your data, so ggplot will recognize it as a categorical variable and display multiple boxplots accordingly.

```{r}
mtcars <- mtcars %>% 
     mutate(cyl = as.factor(cyl))
ggplot(mtcars, aes(cyl, mpg)) + geom_boxplot()

```

#### Try to Recreate

-   You try examples of scatterplots using the UScrime data set that is part of the MASS package to examine a few relationships using ggplot2. A few examples are below.

```{r, echo = FALSE}
library(MASS)
data("UScrime")
ggplot(UScrime, aes(x=Po1, y=GDP)) + geom_point() + geom_smooth(method="lm")
ggplot(UScrime, aes(x=Ed, y=y)) + geom_point() + geom_smooth(method="lm")
ggplot(UScrime, aes(x=Po1, y=y)) + geom_point() + geom_smooth(method="lm")
ggplot(UScrime, aes(x=GDP, y=y)) + geom_point() + geom_smooth(method="lm")
```

-   See what information you can take away from the scatterplots above and create some more to practice.

# Using AI

-   Use the following prompts on a generative AI, like chatGPT, to learn more about data visualization capabilities in ggplot2.

-   How can I modify the appearance of a ggplot bar chart to include custom colors for each bar, and what are the best practices for choosing colors in data visualization?

-   What is the role of layering in ggplot, and how can adding multiple layers, such as labels, themes, and lines, improve the readability of a plot?

-   When should a density plot be used instead of a histogram, and how does each visualization help in understanding the distribution of continuous data?

-   How can I use a boxplot in R to identify and visualize outliers in my dataset, and what additional steps should I take to handle these outliers?

-   How can I create a scatter plot in ggplot to explore relationships between two continuous variables, and how do I add a trendline to help interpret the results?

-   What are the steps for creating a grouped bar chart in ggplot, and how does this visualization help in comparing multiple categories or groups within a dataset?

## Summary

-   Practice many more examples with the help of ChatGPT and work towards constructing high-quality charts and graphs.

